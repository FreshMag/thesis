\documentclass[12pt,a4paper,openright,twoside]{book}
\usepackage[utf8]{inputenc}
\usepackage{disi-thesis}
\usepackage{code-lstlistings}
\usepackage{notes}
\usepackage{shortcuts}
\usepackage{acronym}

\school{\unibo}
\programme{Corso di Laurea Magistrale in Ingegneria e Scienze Informatiche}
\title{Static Analysis for Novel Programming paradigms via Compiler Plugins}
\author{Francesco Magnani}
\date{\today}
\subject{Software Process Engineering}
\supervisor{Danilo Pianini}
\cosupervisor{Nicolas Farabegoli}
\morecosupervisor{Angela Cortecchia}
\session{III}
\academicyear{2023-2024}

% Definition of acronyms
\acrodef{IoT}{Internet of Thing}
\acrodef{vm}[VM]{Virtual Machine}
\acrodef{IDE}[IDE]{Integrated Development Environment}
\acrodef{CI}[CI]{Continuous Integration}
\acrodef{AST}[AST]{Abstract Syntax Tree}
\acrodef{KAPT}{Kotlin Annotation Processing Tool}
\acrodef{KSP}{Kotlin Symbol Processing}
\acrodef{IR}{Intermediate Representation}
\acrodef{DSL}{Domain Specific Language}
\acrodef{IR}{Intermediate Representation}
\acrodef{FIR}{Frontend Intermediate Representation}
\acrodef{PSI}{Programming Structure Interface}
\acrodef{TDD}{Test-Driven Development}


\mainlinespacing{1.241} % line spacing in mainmatter, comment to default (1)

\begin{document}

\frontmatter\frontispiece

\begin{abstract}	
Max 2000 characters, strict.
\end{abstract}

\begin{dedication} % this is optional
Optional. Max a few lines.
\end{dedication}

%----------------------------------------------------------------------------------------
\tableofcontents   
\listoffigures     % (optional) comment if empty
\lstlistoflistings % (optional) comment if empty
%----------------------------------------------------------------------------------------

\mainmatter

%----------------------------------------------------------------------------------------
\chapter{Introduction}
\label{chap:introduction}
%----------------------------------------------------------------------------------------

Analyzing characteristics of the source code without necessarily building and
executing it --- i.e., static analysis --- is a process that has been studied and
implemented in various forms during the last decades. Various tools have been
developed to perform such a task, each with its own strengths and weaknesses.
%
The need for such a tool is often evident in the software development process,
where ensuring the \textbf{quality} and \textbf{robustness} of software systems
is a critical concern. Using code quality analysis techniques is also a powerful
mean to avoid situations of ``technical debt''
\cite{DBLP:conf/sigsoft/ErnstBONG15}, that targets the system quality in
maintenance and evolution.

As the system grows in complexity, so does the variety of errors and
vulnerabilities that can be detected through tools (e.g., concurrency management
issues, error handling, etc.). At the same time, adhering to coding standards
helps to avoid both trivial and non-trivial errors from the outset.
Many tools exist to enforce these coding standards and to detect violations and,
moreover, these are usually among the easiest types of tools to integrate, often
already included within \acp{IDE}.
%
The effectiveness of static analysis tools has been the subject of various
studies,  \cite{DBLP:journals/jss/LenarduzziPSLP23} evaluating their detection
capabilities, agreement, and precision. Some of these studies revealed a low
degree of agreement among the tools and highlighted the need for a better
understanding of their actual capabilities. More advance tools have been used
also to rewrite code and help with the development of very complex systems,
like \emph{Coccinelle} for the Linux kernel
\cite{DBLP:conf/eurosys/PadioleauLHM08}\cite{DBLP:conf/usenix/LawallM18}.
%
More over, in the last years, the static analysis tools have become more popular
and easier to use, becoming protagonists of many \ac{CI} pipelines
\cite{DBLP:conf/msr/ZampettiSOCP17} that automatically performs checks on the
entire source code, embracing change and evolution of the software without
making it a threat \cite{DBLP:books/daglib/0015650}, backed by a solid safety
net. 

\section{Static Analysis for novel paradigms}

Despite all this, these tools are not always available out-of-the-box,
especially in the case of new and experimental language paradigms. Developing
useful static analysis tools means having a \textbf{deep understanding of the
language} used to write the code, as well as \textbf{knowing the paradigm well
enough} in order to reason about the main pitfalls and points of failure present
within it.
%
Beyond this, static analysis tools are inherently difficult to develop, as they
must be exceptionally reliable and robust. They serve as critical foundations
for software development, and they are among the last places where one would
want to encounter a bug. Any flaw in these tools can lead to incorrect analysis,
misguiding developers and potentially introducing severe issues into a codebase.
%
Moreover, from a technical standpoint, developing such tools also requires
building a substantial integration layer with other systems to ensure their
usability. For instance, they need to seamlessly integrate with \acp{IDE}, code
editors, and development pipelines, further increasing the complexity of their
implementation.

Even considering all of this, the challenge of developing these tools extends
beyond technical complexity. For a static analysis tool to be created, the
programming paradigm in question must first reach a sufficient level of
\textbf{interest} and \textbf{usability} within the software development
landscape: only then does the need for such a tool become evident. However, the
growth and adoption of a new paradigm are often inhibited by the very absence of
these tools, which have become essential in most development contexts. This
creates a self-perpetuating cycle: without adequate tooling, a paradigm
struggles to gain traction, yet without widespread adoption, there is little
incentive to develop the necessary tools.
%
As a result, developers and industry stakeholders tend to fall back on
well-established tools with mature and certified ecosystems, reinforcing the
dominance of existing paradigms. This, in turn, leads to significant
\emph{technical debt}, as developers are forced to work within paradigms that
may not be the most suitable for the problem at hand. Beyond that, this
situation also causes many promising research projects to remain confined within
academia, preventing their potential impact on the broader software development
industry. 

It is clear now that knowing how can this cycle be broken is a key point in 
the development of new paradigms and tools.

\section{The role of Domain Specific Languages} \label{sec:dsls}

When implementing a new paradigm in the form of a programming language,
\acp{DSL} are often used. A \ac{DSL} is a specialized language designed for a
specific domain, rather than a general-purpose programming language. Since many
new paradigms do not require extensive general-purpose functionality, \acp{DSL}
often provide a natural and efficient way to express the paradigm’s core
concepts.
%
One of the key advantages of \acp{DSL} is their ability to bring the target
domain closer to its usage and manipulation. This often results in a syntax that
is more intuitive, sometimes even resembling natural language, and encourages a
more declarative way of writing code. Additionally, because \acp{DSL} are
inherently more restricted in scope than general-purpose languages, developing
static analysis tools for them tends to be significantly easier. Their limited
expressiveness reduces the complexity of the analysis, making it more feasible
to create robust and effective tooling. 

Recently, many new \acp{DSL} have been implemented as \emph{internal} to a host
language. Internal \acp{DSL} are embedded within a general-purpose language,
using the host language's syntax and semantics to define the domain-specific
constructs. This approach offers several advantages, including the ability to
leverage the host language's ecosystem and tooling, without having to develop a
compiler from scratch and manage all the necessary checks. 
%
Additionally, internal \ac{DSL} users can use already existing libraries
available for the host language to make their life easier. This approach,
however, also comes with its disadvantages, the main one being the limitations
in their expressiveness, as the host language's capabilities can
\textbf{constrain syntax and semantics}. For this reason, developing internal
\acp{DSL} could suffer from scalability issues when the lack of full control
over the language becomes an obstacle to the paradigm reflection in the code.
Moreover, implementing custom static checks over the code could be difficult
when the host language does not provide the necessary hooks to do so (i.e., 
meta-programming capabilities).

In conclusion, even though internal \acp{DSL} can be a solution to the first
half of the problem --- i.e., the adoption and implementation of the paradigm
with less technical challenges --- they still suffer from the lack of proper
tooling for specific static analysis.

\section{Enabling static analysis through Compiler Plugins}

If looked from another perspective, automated static analysis tools and
compilers share significant similarities in their operations. Both perform
thorough examinations of source code without executing it, aiming to identify
errors, enforce coding standards, and optimize performance. Most of what
compilers do on the static analysis side is to facilitate code optimization and
error detection during the compilation process. In fact, a compiler can be
viewed as a form of static analysis tool, as it analyzes code to generate
executable programs and associated debugging information
\cite{DBLP:journals/queue/Thomson21}.
%
Specialized static analysis tools, on the other hand, extend beyond the
capabilities of standard compilers by offering additional functionalities and
broader diagnostic capabilities. They offer a broader range of diagnostic rules,
enabling the detection of specific and uncommon bugs that compilers might
overlook.
%
What if, however, the compiler were extended beyond its core functionality,
incorporating specialized capabilities that go beyond the scope of a
general-purpose environment? These extensions could be designed to address the
unique requirements of a specific project or domain. This is precisely where
\textbf{compiler plugins} come into play.

Compiler plugins are dynamic modules that interact with the compiler during its
various phases, enabling the introduction of new functionalities or the
modification of existing behaviors. They serve as intermediaries that can
inspect, modify, or enhance the compilation process, providing developers with
the flexibility to implement domain-specific checks, optimizations, or
transformations, all without altering the compiler's core architecture. 
%
For instance, in the context of the GNU Compiler Collection
(GCC), plugins allow for the addition of new features without necessitating
modifications to the compiler itself (mostly, again, for optimization purposes).

\subsection{Types of Compiler Plugins}

Compiler plugins can be broadly categorized based on the phase of compilation they target:

\begin{itemize}
  \item \textbf{Frontend Plugins}: These plugins operate during the initial
  stages of compilation, focusing on tasks such as syntax analysis, semantic
  analysis, and \ac{IR} generation (i.e., an internal data structure used by the
  compiler). They are useful also for implementing custom syntax extensions,
  enforcing coding standards, or performing static code analyzes. For example,
  in the \emph{Rust} programming language, compiler plugins can introduce new
  syntax extensions and lint checks. 

  \item \textbf{Backend Plugins}: Functioning in the latter stages of
  compilation, backend plugins are concerned with code optimization, machine
  code generation, and platform-specific adjustments. They can be utilized to
  implement custom optimizations, support additional hardware
  architectures and more.
\end{itemize}

\subsubsection{Compiler Plugins in Kotlin}

Kotlin, a statically typed programming language developed by JetBrains, offers
robust support for compiler plugins, allowing developers to highly customize the
compilation process to their specific needs. 
%
Kotlin's compiler architecture facilitates the creation of plugins that can
modify or extend its behavior during compilation. For example, the
\emph{all-open} compiler plugin in Kotlin allows classes annotated with a
specific annotation to be open without the explicit \lstinline{open} keyword,
adapting Kotlin to the requirements of frameworks that need classes to be
open. 

When guiding developers towards the creation of compiler plugins, JetBrains
compares them to \textbf{Annotation Processors}
\cite{JetBrains:KotlinCompilerPlugin}. Annotation processors are a powerful
feature in many modern programming languages, including Java and Kotlin, that
allow developers to generate code, validate code, and perform various
compile-time checks based on \textbf{annotations} present in the source code.
%
In Java, annotation processors are part of the Java Compiler API and can be used
to generate additional source files, validate the correctness of the code, and
even modify the \ac{AST} of the code being compiled. They are
commonly used in frameworks and libraries to reduce boilerplate code and enforce
coding standards.
%
Kotlin also supports annotation processors through the \ac{KAPT} --- which is, in
fact, a compiler plugin itself --- that allows Kotlin code to interoperate with
Java annotation processors and, more recently, the \ac{KSP}, another compiler
plugin introduced as ``an API that you can use to develop lightweight compiler
plugins''. The former enables developers to leverage existing Java annotation
processors in their Kotlin projects and the latter, on the other hand, provides
a more efficient and Kotlin-specific way to generate code at compile time,
offering a new approach that is much more integrated with Kotlin symbols.

\subsection{Advantages of Compiler Plugins}

Compiler plugins and Annotation Processors, however, have some very distinct
functionalities. While Annotation Processors are limited to generating source
code and performing checks based on annotations, compiler plugins can exploit a
very powerful API that can create and modify byte-code, elements inside the
\ac{IR} and more, allowing the developers to solve a whole new class of
meta-programming problems. In addition, the use of annotations in the code could
make it more ``cluttered'' from the perspective of a DSL, reducing it to
constructs that belong more strictly to the host language, while the compiler
plugin would not need these annotations. Of course, Annotation Processors are
typically easier to write and maintain than compiler plugins, but this extra
cost can be worth in several cases, for example in the scenario that will be
presented in this thesis.

\subsection{Main challenges and requirements}

At the time of writing, the development of frontend compiler plugins in Kotlin is
still a relatively less explored area compared to the backend ones. Because of the 
very little documentation and examples available, the development of frontend 
compiler plugins can be a challenging task, that quite often requires inspecting 
the Kotlin compiler source code to understand how to interact with it.
%
Frontend compiler plugins can be implemented using \textit{Extensions} to the
Kotlin compiler, a topic that will be covered more in detail in the following
chapters. This thesis presents the development process of a frontend compiler
plugin designed to build upon an existing \textit{backend} plugin. The primary
purpose of this frontend plugin is to perform static checks on source code,
ensuring compliance with specific rules related to the functionality of the
pre-existing backend plugin. To better understand the target rules developed
within this frontend plugin and its context, it is necessary to first introduce 
the backend plugin and the project it is part of.

\paragraph{Structure of the Thesis}

This thesis follows the development process of a frontend compiler plugin from
its initial steps, addressing the key challenges encountered as well as the
solutions proposed during the research and implementation phases. The next
chapter, \cref{chap:background}, provides an overview of the ongoing project for
which this frontend plugin is being developed, alongside the technical
background necessary to understand how a plugin can interact with the Kotlin
compiler.
%
\Cref{chap:contribution} delves into the core development process of the plugin,
discussing the design decisions, alternative approaches considered, and the
final implementation of the proposed \textbf{checkers}. Subsequently,
\cref{chap:evaluation} evaluates the plugin’s behavior, with a particular focus
on the testing methodology adopted, including the integration of a custom
testing framework named \emph{Subjekt}, developed specifically for this purpose.
%
Lastly, \cref{chap:conclusion} summarizes the primary contributions of this
thesis and outlines potential directions for future work, building on the
results and insights gained throughout this research.

%----------------------------------------------------------------------------------------
\chapter{Background: the Collektive case}
\label{chap:background}
%----------------------------------------------------------------------------------------

As already stated, the development of the frontend compiler plugin presented in
this thesis will be built on top of an existing backend plugin. The backend
plugin is part of a larger project named \emph{Collektive}, a Kotlin
multiplatform framework that provides a \ac{DSL} for the \textbf{Aggregate
Computing} \cite{Beal2015} paradigm. This chapter will provide an overview of
the concepts behind Collektive as well as the feature behind its backend plugin
and Kotlin compiler plugins development in general.

\section{Aggregate Computing: a novel paradigm}

The Aggregate computing paradigm is a novel approach to ``design, create and
maintain'' \cite{Beal2015} complex distributed systems, particularly in the 
context of the Internet of Things (IoT). The paradigm shifts the focus from
individual devices to regions of devices, abstracting away the details of their
number, position, and behavior. This abstraction enables developers to reason
about distributed systems in terms of \emph{collective} operations over spatial and
temporal fields, rather than device-to-device interactions. 

The foundation of aggregate programming is built on \textbf{field calculus}
\cite{Beal2014TowardsAU}, a set of constructs designed for \emph{spatial
computing}. These constructs enable the implementation of robust coordination
mechanisms, such as self-stabilizing and scalable operations \cite{Viroli2018}
\cite{DBLP:journals/jlap/ViroliBDACP19}, which adapt dynamically to changes in
the environment. Applications of aggregate programming are particularly
impactful in large-scale scenarios, such as crowd management during public
events, where distributed devices coordinate to provide services like crowd
density estimation, dispersal advice, and navigation support.

Aggregate computing formalism has been proposed in various ways, introducing
syntaxes and semantics to support distributed, collective behaviors in dynamic
systems. Field calculus, as a foundational model, supports aggregate computing
by enabling global-level manipulation of computational fields through a
minimalistic syntax. 
%
Tools like Protelis \cite{DBLP:conf/saso/PianiniBV17}, ScaFi
\cite{DBLP:conf/ecoop/CasadeiV16} and the recently proposed eXchange Calculus
(XC) \cite{DBLP:journals/jss/AudritoCDSV24} extend field calculus principles,
providing programming frameworks and language constructs to bridge the gap
between theoretical models and practical implementations.

Managing the programming line of action for these kinds of systems is not
trivial, but several approaches have already been proposed, including static and
dynamic checks that can spot subtle bugs and vulnerabilities in the code, by
focusing on concepts like \textbf{neighborhood interactions} and \textbf{domain
alignment} \cite{DBLP:conf/saso/AudritoDVC16}.

\subsection{Applications and critical aspects}

In Aggregate Computing, the main model of the system consists of a \emph{network of 
intercommunicating devices}, which can be \emph{close} to one another therefore 
introducing a concept of \textbf{neighborhood}. 
%
Each device is equipped with sensors and actuators, enabling interaction with an
environment, and can communicate with other devices through a
\emph{message-passing} system. 

A key aspect of Aggregate Computing regards the execution model, which is based
on a local program identical for all devices. The system is governed by a
continuously executed loop that makes the devices \emph{receive messages},
\emph{produce a result through their internal behavior} and finally \emph{send
values to neighbors}. 
%
This simple structure allows for system to show self-organizing behaviors
emerging from the \emph{collective} of devices, and contemplates even complex
interactions of the devices with the environment (e.g. creation of new devices).

Finally, as already said, the \emph{field calculus} model can be implemented
through an ad-hoc API and syntax to perform operations on a \emph{computational
field} (i.e., a mapping from device locations to values in our example)
manipulating it over time, defining interactions between devices and finally
creating the emergent behavior.
%
After the previous tools already cited, it is now necessary to
introduce the Aggregate Computing framework that will be the target of the
frontend plugin in this thesis: \emph{Collektive}.

\section{Collektive: an Aggregate Computing framework}

\emph{Collektive} is a modern Aggregate Computing framework developed in Kotlin 
that allows developers to easily write Aggregate Computing programs through a 
flexible \emph{internal} DSL. The framework is designed to be multiplatform,
targeting both JVM and JavaScript platforms, as well as native ones. 
%
Compared to some other existing Aggregate Computing frameworks, Collektive
offers a modern and idiomatic approach to writing Aggregate Computing programs,
with a static type system (as it is internal to Kotlin) and few, expressive
constructs like \lstinline{neighboring} and \lstinline{exchange}, which can be
used to implement a broad variety of interactions between devices (and also
Aggregate Computing patterns).

The project is organized in modules, with the main one being the \lstinline{dsl}
and \lstinline{compiler-plugin}, and is tested using the Alchemist simulator
\cite{DBLP:journals/jos/PianiniMV13}. 

\subsection{Collektive DSL: main concepts} \label{sec:collektive-dsl}

In order to understand how to work on Collektive programs, we first need to
understand its main usage. Collektive DSL is centered around the
\lstinline{aggregate} function, which is the entry point for the Aggregate
Computing program. The function uses a local ID to identify the device on which
is executed on and then accepts a function type with receiver
\lstinline{compute: Aggregate<ID>.() -> R} that performs the aggregate
computation using the \lstinline{Aggregate} interface members.
%
The \lstinline{Aggregate} interface provides several functions that represent
the main constructs of Collektive's implementation of the Aggregate Computing
paradigm. The most important ones are:
\begin{itemize}
  \item \lstinline{neighboring}: a function that observes expressions on
  neighbors, returning the related field;
  \item \lstinline{exchange}: a function that, taken from the documentation of
  Collektive, ``manages the computation of values between neighbors in a
  specific context''. In practice, it can be used to compute a new field of 
  values starting from initial values of neighbors;
  \item \lstinline{evolve}: updates an initial value iteratively computing an
  expression at each device;
  \item \lstinline{share}: performs a field computation starting from neighbors'
  values, reducing them to a single value and then sharing it with neighbors. 
  It can be used to actuate a ``space-time evolution'' on the field.
\end{itemize}

Some of these constructs is present also in a variant that allows to return a
different type of value from the initial one; the name of the variant is 
the same of the original function but with the suffix \lstinline{-ing} (e.g., 
\lstinline{exchanging}, \lstinline{evolving} etc.).

Just by using these four constructs, already complex collective behaviors can
be achieved. Besides that, the Collektive DSL hides several operations through
the already mentioned compiler plugin. 

\subsection{Collektive Compiler Plugin}

As previously introduced, Collektive provides an already integrated
\textbf{backend compiler plugin}. This plugin is responsible for managing the
\emph{alignment of aggregate computation}, a very important concept when dealing
with Aggregate computing programs, faced in several studies
\cite{DBLP:conf/forte/DamianiVPB15} \cite{DBLP:conf/saso/AudritoDVC16}.
%
Essentially, the domain alignment is a necessary step to ensure that when a
device computes a value that depends on neighboring devices (e.g., through the
\lstinline{neighboring} construct, which retrieves values from neighbors), those
neighboring devices have computed the same expression in the same evaluation
round. This guarantees that shared computations remain consistent across
devices: it's necessary in order to \textbf{maintain consistency between field
values} and \text{prevent information leakage} because ``without it, information
may leak unexpectedly between devices that are evaluating different functions,
or may be “blocked” from passing between devices evaluating the same function''
\cite{DBLP:conf/forte/DamianiVPB15}.

Collektive \emph{backend} compiler plugins works by analyzing call sites and
function definitions in the code, intercepting the ones that involve aggregate
computation and that should, therefore, be ``aligned''. Essentially, this is
done by looking at \lstinline{Aggregate} interface usage in functions,
especially when used as receiver, and visiting their declarations, wrapping
aggregate constructs usages with special \lstinline{align} and
\lstinline{dealign} functions that perform domain alignment under the hood.
%
As we will see, this is not always automatic or safe in certain situations 
(e.g., loops), and the frontend extension will take care of these extra 
checks. 

But how do Kotlin compiler plugins work in general? Before diving into the 
core development of the frontend plugin, it is necessary to understand their
main structure and how they interact with the Kotlin compiler.

\section{Kotlin Compiler Plugins: general structure} 

Since Kotlin is a multiplatform language, the same source code can be compiled
into low-level code specific to different targets, such as the JVM, JavaScript,
and native platforms. In order to work with different targets, the Kotlin
compiler architecture is divided into two sub-parts: the \emph{frontend} and the
\emph{backend}\footnote{The following explanation is greatly
inspired from the work of \cite{moskala2023}, which provides a more
comprehensive overview of the Kotlin compiler plugins architecture.}.
%
The frontend is independent of the target, and for this reason its output can be 
reused when targeting different platforms. Currently, this part of the compiler 
is being migrated to the new K2 version, which is supposed to be much more 
efficient than the previous K1 version.
%
The backend, on the other hand, is mostly specific to the target platform, and
uses the output of the frontend to generate the final code. In reality, the
backends for JVM, JS, Native, and WASM share some parts: that will be analyzed
later. The general structure is summarized in
\cref{fig:kotlin-compiler-architecture}.

\begin{figure}
  \centering
  \includegraphics[width=.8\linewidth]{figures/kotlin-compiler-architecture.pdf}
  \caption{Kotlin compiler architecture. Adapted from the one at \cite{moskala2023}}
  \label{fig:kotlin-compiler-architecture}
\end{figure}

\subsection{Kotlin K2 and frontend plugins}

The frontend's output is not only used by the backend for the final one, but it
is also responsible for communicating with \acp{IDE} and build tools, providing
APIs to present errors, warnings, code completions and so on. 
%
To make this architecture modular, the Kotlin compiler provides a set of 
\acp{IR}, that the various steps of the workflow
can use to process the preceding steps' output. Both the frontend and the
backend creates this data structure, although they are very different. The 
backend's one is created starting from the output \ac{IR} of the frontend,
while the frontend's one is created from the Kotlin source code. 
The general workflow is summarized in \cref{fig:kotlin-compiler-workflow}.

\begin{figure}
  \centering
  \includegraphics[width=.9\linewidth]{figures/kotlin-compiler-workflow.pdf}
  \caption{Kotlin compiler general workflow. Adapted and extended from the one
  at \cite{moskala2023}}
  \label{fig:kotlin-compiler-workflow}
\end{figure}

Before the so-called K2 frontend the compiler's frontend worked by
building the \ac{PSI}, a syntactic model of the parsed source code, and the
\lstinline{BindingContext}, that holds semantic information such as types and
symbol bindings (represented in \cref{fig:kotlin-compiler-workflow} as a whole). 
%
The new K2 frontend, on the other hand, builds the \ac{FIR}, a more powerful and
complete representation of Kotlin parsed code, capable of offloading some of the
work that was previously done by the backend and enabling powerful optimizations
and caching mechanisms. The raw \ac{PSI} is still being produced, but it is now
transformed into the \emph{raw FIR}, which again transforms in different stages,
filling the tree with semantic information. Finally, the resolved tree is passed
to the backend, which takes care of the platform-specific (backend) \ac{IR}, used
to generate the final code.

As we can see in \cref{fig:kotlin-compiler-workflow}, there is still another
process that hasn't been mentioned yet: the \emph{checkers}. During their stage
of execution, the checkers can inspect the \ac{FIR} and reports different
diagnostics. If some of them is considered ``critical'', in the sense that the
compilation should not proceed (e.g., a type error), the compilation is stopped,
and the backend is not executed: otherwise, the final \ac{IR} is passed to it.
%
The role of the checkers is particularly important in the context of this
thesis, and the way it's possible to interact with them is by means of
\textbf{extensions}.

\subsection{Extension mechanism}

Kotlin compiler extensions are mechanisms that allow developers to modify
various phases of the compilation process, either by analyzing and transforming
code at the \ac{FIR} level or by modifying the backend \ac{IR}. These extensions
enable advanced features such as additional type checks, automated code
generation, and optimizations, empowering Kotlin’s extensibility.
%
Like with compiler stages, extensions can be either frontend or backend as well.
While frontend extensions impact code analysis, syntax resolution, and IDE
support, backend extensions act right after the \emph{IR generation +
optimization} phase seen in \cref{fig:kotlin-compiler-workflow}. This
distinction makes frontend extensions more suitable for language-level changes
and linting rules, whereas backend extensions are primarily used for performance
optimizations and bytecode transformations.

K2 introduces multiple frontend extensions, all following the
\lstinline{Fir[Name]Extension} convention. In relation to what was previously
discussed, the \lstinline{FirAdditionalCheckersExtension} is particularly
important in this context, since it's a frontend extension that allows
developers to register \emph{additional checkers} to run during compilation.
These checkers can enforce custom coding rules, report warnings, or even prevent
compilation by issuing errors. More over, errors and warnings generated by this
extension appear in IDEs like IntelliJ IDEA, improving real-time code feedback.
This extension is used by Kotlin plugins like \emph{Kotlin Serialization},
which ensures that serialization-related constraints are properly followed, and
\emph{Arrow Meta}, which enforces functional programming best practices.

The extension mechanism is also used in the backend, where backend plugins can
intervene, but the extension that needs to be used in only one:
\lstinline{IrGenerationExtension}. This extension is invoked after the \ac{FIR}
phase has completed and the \ac{IR} has been generated. It allows modifications
to the IR tree before it is used for bytecode generation. Because IR sits
between the frontend and the platform-specific backend, changes made here can
impact the generated machine code without altering the high-level source
representation --- and, for this reason, it does not influence code analysis
in \acp{IDE} like IntelliJ IDEA. This extension is widely used in performance-critical
applications. For instance, \emph{Jetpack Compose} leverages it to transform
composable functions into an optimized internal representation. Similarly,
\emph{Kotlin Serialization} uses it to generate serialization methods dynamically,
ensuring they are both efficient and lightweight. However, modifying IR directly
can introduce breaking changes if not handled carefully. Since IR
transformations occur at a low level, even minor alterations can lead to
unintended consequences, making this extension a powerful but complex tool.

\subsubsection{IR generation inside Collektive} \label{sec:ir-generation-collektive}

Collektive backend plugin uses the \lstinline{IrGenerationExtension} to perform 
the operations we already discussed. To register an extension, the developer needs
to declare a class that extends \lstinline{CompilerPluginRegistrar}. Inside that,
the developer can register the extension using the \lstinline{registerExtension}
method that depends on the type of extension that is being registered. 
%
The core login of the plugin is then implemented in the extension itself. In the
case of the Collektive backend plugin, the class structure is summarized 
in \cref{fig:extensions-class-diagram}.

\begin{figure}
  \centering
  \includegraphics[width=.9\linewidth]{figures/extensions-class-diagram.pdf}
  \caption{Class diagram representing the top-level structure of the Collektive 
  backend compiler plugin. Note: the frontend plugin is not included yet.}
  \label{fig:extensions-class-diagram}
\end{figure}

Finally, to include the plugin, a \lstinline{META-INF/services} file is needed,
where the fully qualified name of the \lstinline{AlignmentComponentRegistrar} is
written, and the plugin is wrapped in a Gradle plugin that can be applied to the
project build. After the application of the frontend plugin also, the structure
of the project's components will be the one described in
\cref{fig:collektive-components-diagram}.

\begin{figure}
  \centering
  \includegraphics[width=.6\linewidth]{figures/collektive-components.pdf}
  \caption{Components of the Collektive project after the application of the frontend plugin}
  \label{fig:collektive-components-diagram}
\end{figure}

\section{DSL and Compiler Plugins}

In relation to what stated in \cref{sec:dsls}, the feature that is missing in
most \emph{internal} DSLs is the ability to perform static analysis on the code
that is written, and this is particularly difficult since the developers
struggle to intervene in the compilation process of the host language.
%
Compiler plugins seem perfect for this task since, in the case of Kotlin
compiler plugins at least, they can greatly influence the compilation process on
the frontend side and take care of the static analysis of the code. Collektive
is in fact internal to Kotlin and its main pitfalls --- and the ones of
Aggregate Computing in general --- which are currently not captured can be
spotted by a frontend plugin made for the job. 

In other words, the frontend plugin can take care of the ``other half of the 
problem'' introduced in \cref{sec:dsls}.

\subsection{The importance of Build Tools}

Considering all of this, it is important to mention the role of build tools in
the development process of a compiler plugin. Without a seamless integration of
the compiler plugin during the compilation process (and, as it will be shown,
the testing process as well), the development of a compiler plugin would be
cumbersome and error-prone. 
%
Needless to say, a well-configured build tool is a fundamental requirement in
this context. Within the Kotlin ecosystem --- on which this work focuses --- the
most widely used build tool is \emph{Gradle}, and it will serve as the
foundation for the implementations discussed in this thesis.

\subsection{Main motivations}

Before diving into the development of the frontend plugin, let's briefly
summarize the motivations behind the project. As previously mentioned, Aggregate
computing is still a relatively new paradigm, and some studies
\cite{DBLP:conf/saso/AudritoDVC16} already pointed out subtle bugs that can
occur when using some of its possible implementations. 
%
Collektive is not an exception and, in this particular case, developing a
frontend compiler plugin is motivated by three main reasons:
\begin{enumerate}
  \item \textbf{The backend plugin}: since a backend plugin is already present
  and necessary for the correct functioning of the framework, the frontend
  plugin can be simply built on top of it, extending its functionalities and
  ensuring that the user's operations are safe and within the scope of
  Collektive DSL correct usage principles.

  \item \textbf{The presence of an internal DSL}: when building a DSL, a
  developer must choose to make it \emph{internal} to a host language, like
  Kotlin, or \emph{external}, like Protelis \cite{DBLP:conf/saso/PianiniBV17},
  and so take care of the parsing and the compilation process in general.
  %
  As already said, internal \acp{DSL} are typically easier to use and to
  develop, but they can also be less flexible and more error-prone, because the
  developer does not have full control over the parsing process and therefore
  cannot always enforce the constraints that the DSL should have. This is the
  case with Collektive, and the frontend plugin can help in this regard.
  
  \item \textbf{The integration with the other development tools}: developing a
  frontend plugin integrates very well with the other tools that are already 
  used in most projects, for example:

  \begin{itemize}
    \item \textbf{IDE}: since the IntelliJ IDEA \ac{IDE} is the most indicated
    for Kotlin development and it has a very good support for the Kotlin
    compiler, the frontend plugin can leverage this support to provide real-time
    feedback for diagnostics and errors it produces, without the need of
    developing specific \ac{IDE} plugins.

    \item \textbf{Build tools}: the Gradle plugin that is used to apply the
    compiler plugin is pretty simple and needs almost zero modifications when
    extending the compiler plugin, differently from what would happen in the
    case of Gradle plugins wrapping external tools (e.g. \emph{ktlint}). This
    means that it can be easily maintained and integrated into other Gradle
    projects with low effort.

    \item \textbf{Rest of the pipeline}: the frontend plugin can be developed 
    very closely to the rest of the Collektive framework through submodules,
    adding dependencies between subprojects in a very clean and maintainable
    way.
  \end{itemize}
\end{enumerate}

Considering these motivations, we can now proceed with the actual development in
the next chapter.

%----------------------------------------------------------------------------------------
\chapter{Frontend plugin development}
\label{chap:contribution}
%----------------------------------------------------------------------------------------

In this chapter we will present the development process that lead to the creation of 
the frontend plugin inside the Collektive project.

\paragraph{Structure of this chapter}

In this chapter, we will first present the general workflow of the plugin's
checkers, then we will proceed presenting several \textbf{patterns} that were
detected observing the Collektive DSL codebase. These patterns represent bad or
inappropriate use cases of the Collektive DSL that are \textbf{not} captured by 
default. 
%
The way these patterns will be presented does not follow the chronological order 
of their development, but rather an order with an increasing level of complexity.
For this purpose, the patterns will be categorized into four main groups, 
reflecting the design decisions that were taken to approach the related problems,
highlighting the pros and cons of each approach. 

\textbf{Note}: the features of the Kotlin compiler explained and shown in
this chapter are \emph{experimental}, and therefore subject to possible instability
and frequent changes. The code snippets and examples provided are based on the
current state of the Kotlin compiler at the time of writing. 

\section{Adopted workflow}

After defining the \lstinline{CollektiveFrontendExtensionRegistrar} class that
wraps the frontend plugin, already introduce in \cref{sec:ir-generation-collektive}, the
first step is to register the actual extensions to the compiler using the
\lstinline{configurePlugin} method. For this plugin, the extension that will
be used is the already cited \lstinline{FirAdditionalCheckersExtension}, which
allows to register additional checkers to run during compilation.
%
This extension can contain an arbitrary number of checkers, each of them
assigned to different elements of the \ac{FIR} tree (e.g., expressions,
declarations etc.). All the checkers that will be presented are added 
through this extension. The final class structure is summarized in
\cref{fig:frontend-class-diagram}.

\begin{figure}
  \centering
  \includegraphics[width=.8\linewidth]{figures/frontend-class-diagram.pdf}
  \caption{Class diagram summarizing the structure of the frontend plugin}
  \label{fig:frontend-class-diagram}
\end{figure}

From this point on, the workflow of the plugin development will be as follows:

\begin{enumerate}
  \item \textbf{Pattern detection}: when a new pattern that needs to be captured
  by the plugin is detected, several examples of Collektive programs expecting a
  positive or negative  diagnostic are written (i.e., respectively, cases in
  which the diagnostic should be reported and cases where it shouldn't).
  \item \textbf{Test arrangement}: the examples previously written are used to 
  create a test suite that will be used as certification of the pattern's correct
  capture., following a \ac{TDD} approach.
  \item \textbf{Checker creation and implementation}: a new checker is created and
  added to the frontend extension, implementing the logic that will detect and 
  correctly report positives of the pattern's usages.
  \item \textbf{Adjustments and identification of corner cases (optional)}: during
  the checker's development some corner cases may arise that need to be handled 
  by this checker and added as regression tests. In this case, after the corner 
  cases are correctly identified, this goes back to step 2, working on the already
  created checker.
\end{enumerate}

In this chapter we will present these steps for each pattern except for the step
2, that will be covered in \cref{chap:evaluation}. 

\textbf{Note}: the code snippets that will be presented in the following
sections, if not appropriately specified in the code, present portions of code
that are \textbf{inside an Aggregate block}, therefore contained either in a
function that has the \lstinline{Aggregate} interface as receiver or in a 
\lstinline{aggregate} entry point block, since these are the places where 
the Collektive DSL can be used to specify aggregate behaviors. This has been
done for the sake of brevity and clarity.

\section{1st approach: direct Kotlin API usage}

The API that Kotlin provides to interact with the \ac{FIR} is sufficiently
powerful to be used to perform a wide range of operations and checks. The
first approach to the development of the checkers was to use this API directly,
without the need of any additional constructs or classes. This approach is the
most direct and simple, but it was used only for the simplest pattern, without a
very complex logic behind it.

\subsection{Pattern 1: explicit align/dealign}

One of the first pattern detected was the explicit usage of the
\lstinline{align} and \lstinline{dealign} functions of the Collektive DSL. These
functions are not supposed to be used directly by the developer because they are
already managed by the backend plugin, and their direct usage can lead to
inconsistent behavior and unexpected results. For how the Collektive DSL is
structured, it was not possible to prevent the usage of these functions directly
through the API, so the frontend plugin was put in charge of this task.

\subsubsection{Formal description}

Given two functions \lstinline{align} and \lstinline{dealign} available in the
\lstinline{Aggregate} interface, the pattern is satisfied when any of these
functions are used explicitly in the code. \Cref{lst:p1-example} shows an
example of this pattern.

\lstinputlisting[
  float, 
  language=Kotlin, 
  label={lst:p1-example},
  caption={Example of Pattern 1 detection in code}
]{listings/P1Example.kt}

\subsubsection{Design and Implementation}

One of the type of checkers available among \ac{FIR} checkers is the
\lstinline{FirFunctionCallChecker}, which is just a type alias for a
\lstinline{FirExpressionChecker} --- that is, a checker that takes care of
expressions in the code --- typed with a \lstinline{FirFunctionCall} --- i.e.,
an element of the \ac{FIR} tree representing a function call in the code. This
type of checker can inspect function calls usages, calling the \lstinline{check}
method of the checker with a \lstinline{FirFunctionCall} as parameter. 
%
To performs checks on the function call, we can inspect the properties of this
parameter. In this case we simply need to \textbf{compare the fully qualified
name} of the function with the one of the interested functions.

Getting the fully qualified name of the function is not directly supported by
the \ac{FIR} API, but it is possible to build a simple utility function that
retrieves it. The final implementation of the checker is shown in
\cref{lst:p1-checker}: some portions of the code are omitted for brevity.

\lstinputlisting[
  float, 
  language=Kotlin, 
  label={lst:p1-checker},
  caption={Implementation of the Pattern 1 checker: \lstinline{ExplicitAlignDealign}}
]{listings/P1Checker.kt}

The \lstinline{context} object is an instance of \lstinline{CheckerContext} and is 
used to provide contextual information to the checker.

The \lstinline{reportOn} method of the \lstinline{reporter} object is used to
report a diagnostic when the pattern gets detected. The parameter passed to this
method determine where the diagnostic is gonna be reported (i.e., the position
in the code) and the message shown to the user (in these checkers, specified
inside an object called \lstinline{FirCollektiveErrors}).

\section{2nd approach: declarative and modular API}

Even though the API available in the checker is flexible and already powerful,
it can be quite verbose if we need to implement utilities even for simple
operations like obtaining the fully qualified name of a function. To simplify
the development of the checkers and make them more modular, a small API was
developed to make declarative checks on the \ac{FIR} tree. This API is composed
of a set of functions with receiver that can be used to perform common
operations on the \ac{FIR} elements, for example to check if a Checker context
is \textbf{inside an Aggregate function or block}. Some of these utilities are
shown in \cref{lst:simple-api}.

\lstinputlisting[
  float, 
  language=Kotlin, 
  label={lst:simple-api},
  caption={Some utility functions of the simple API developed}
]{listings/SimpleApi.kt}

As we can see, we can perform checks on the \ac{FIR} tree using built-in methods
like \lstinline{containingElements}: we can use this method to check if a
\lstinline{FirElement} is contained inside a function that has a \emph{receiver
parameter} of a certain type, in this case the \lstinline{Aggregate} type, by
inspecting the name of the \emph{Class-like Symbol} (i.e., the symbol referred
to a class or similar entities like interfaces) to which it is related. 
%
Encapsulating this not really trivial operations in functions makes them still
reusable and declarative in the context of domain-specific checkers like the
ones we are developing. This greatly empowers the Kotlin \ac{FIR} API, making it
more interesting also for more complex cases that will be presented later.

\subsection{Pattern 2: simple aggregate operations in loops}

The second pattern that was inspected is the usage of \emph{aggregate operations
inside loops}. This pattern is more complex because it reflects cases that are
not always inappropriate usages, but if not properly handled can lead to
problems during the alignment phase. 
%
When an aggregate construct (i.e., a function call that needs alignment between
devices) is called inside a loop (e.g. a \lstinline{for} loop), the alignment of
the computation fails because multiple devices are not able to align to the same
``instance'' of the call between the iterations. 
%
However, the alignment can succeed if the loop contains a \emph{custom alignment
operation}, that can be done using the \lstinline{alignedOn} method, which is
available inside the \lstinline{Aggregate} interface. This method accepts an
anonymous function that will be the subject of the alignment. 
%
Again, this behavior cannot be captured through the internal DSL alone, so the
frontend plugin is needed to handle this case. 

It could seem that this pattern is not overly complex to detect compared to the
previous one, but in reality just adding one more simple constraint makes the
range of possible cases that can be captured following this pattern much broader
and more varied. Consider the cases presented in \cref{lst:p2-corner-case}. 
\lstinputlisting[
  float, 
  language=Kotlin, 
  label={lst:p2-corner-case},
  caption={Corner case related to Pattern 2, where the construct is used inside 
  a nested function}
]{listings/P2CornerCase.kt}
These corner cases are exceptions to the general rule that we previously stated,
in fact:
\begin{itemize}
  \item In the first case, although, technically, the construct is ``present''
  inside the loop, it is not directly called by the loop itself, but by a nested
  function. Note that even though this can be seen as a corner case, this is
  valid Kotlin code and might appear, maybe as slightly modified versions of
  this one, in real use cases.
  \item In the second case, we have the required \lstinline{alignedOn} operation
  wrapping the Aggregate function call, but it is placed outside the loop,
  making the alignment operation useless.
  \item In the third case, the loop is done without alignment, but this is in
  fact correct because the construct is not iterated by the loop since a new
  aggregate instance is created at each iteration. This makes each of them
  like a separate Aggregate program, and the alignment is not necessary.
\end{itemize}

Many more cases regarding this pattern can be found, but some of these will be
presented as another, separated pattern in \cref{sec:p6}. For now, we will deal
with the simpler cases, described in the following, more formal description.

\subsubsection{Formal description}

Given a construct that loops through various iterations (e.g., a \lstinline{for}
statement, an anonymous function called when cycling collection elements like
\lstinline{map}, \lstinline{forEach} etc.) \emph{inside} an ``aggregate'' block,
the pattern is satisfied when an aggregate construct is used under one of the
following conditions:
\begin{enumerate}
  \item The construct is called inside this loop without a wrapping
  \lstinline{alignedOn} operation \emph{inside the loop} as well;
  \item The construct is called inside this loop as part of a static declaration
  (e.g., a nested function) whose use is not iterated by the loop.
\end{enumerate}

\subsubsection{Design and Implementation}

The description might still seem incomplete or vague to some extent, and the
reason is that some potential cases are still not captured. For now, though, the
developed checker will be able to capture the most common cases, and the rest,
as we will see, will be easily integrated when covering the Pattern 6. 

One possible way to approach this pattern is reasoning in terms of
\textbf{containing blocks}: starting from the aggregate function called, we
could be able to determine if the pattern is detected or not just by looking at
the \textbf{sorted list of containing blocks of the function call}. Starting
from there, we first inspect if a loop is, in fact, containing the function
call, and if so, we check for other elements like the presence of the
\lstinline{alignedOn} in the correct order. This procedure is summarized in
\cref{fig:p2-flowchart}.

\begin{figure}
  \centering
  \includegraphics[width=.6\linewidth]{figures/p2-flowchart.pdf}
  \caption{Flowchart representing the procedure to detect Pattern 2}
  \label{fig:p2-flowchart}
\end{figure}

As we can see from the flowchart, the procedure ends immediately if the function
name is one of \lstinline{alignedOn}, \lstinline{align} or \lstinline{dealign},
since these are functions that do not require alignment. Thanks to Kotlin 
nullability system, we can perform these subsequent checks in a declarative way,
adding utilities to the simple API that we previously introduced and making it 
more extensible for more checks like this.
%
We first add these simple functions shown in \cref{lst:p2-api} to the API, and
then we can finally implement the checker, as shown in \cref{lst:p2-checker}. 

\lstinputlisting[
  float, 
  language=Kotlin, 
  label={lst:p2-api},
  caption={Utility functions added to help in the detection of Pattern 2}
]{listings/P2SimpleApi.kt}

\lstinputlisting[
  float, 
  language=Kotlin, 
  label={lst:p2-checker},
  caption={Implementation of the Pattern 2 checker: \lstinline{NoAlignInsideLoop}}
]{listings/P2Checker.kt}

As we can already see, implementing a checker with this approach has several
advantages: the code is readable and modular, and the logic is
declarative and easier to understand. This approach is also extensible and
can be easily adapted to other patterns that require similar checks.
%
This is, however, a very particular case that can be caught simply by looking at
containing elements. Once the pattern becomes more complex, adding concepts like
\emph{symbol's usage references} or \emph{structured operations with particular
requirements}, like nested anonymous calls or simply subsequent statements that
appear in the code that need to be checked together, this approach quickly
becomes less effective and more verbose, as it requires a very effective and
carefully designed API that cover as many cases as possible. 
%
It becomes quite clear that covering corner cases and patterns that reflect many
possible \ac{FIR} trees needs an approach that is more suitable to this kind of
data structures.

\section{3rd approach: visitor pattern}

One of the main design patterns that can be used to traverse a tree-like data 
structure is the \emph{visitor pattern}. This pattern is particularly useful 
when the tree structure is complex and the operations that need to be performed
on it are varied and not easily encapsulated in a single class.
%
The visitor pattern well fits the context of the Kotlin \ac{FIR}, since the
structure resembles the one of the \ac{AST} built by the Kotlin parser. 
Visitors are naturally used during the compilation process even if we don't
explicitly define any, since the compiler itself needs to traverse the tree to
perform the various operations that are needed to compile the code.
%
Moreover, Kotlin provides a set of utilities that can be used to implement
Visitors that fit the developer needs and that can be easily integrated into the
checkers in order to explore the \ac{FIR} structure behind the
\lstinline{FirElements} that are passed to the checkers.

Kotlin \ac{FIR} visitors are automatically generated inside the Kotlin compiler 
code in order to have a specific method for each type of \lstinline{FirElement}
that can be used to visit that element. All that is left to the developer is to
choose which type of visitor to extend in order to implement their own:

\begin{itemize}
  \item \lstinline{FirVisitor<R, D>}: a visitor where each visit method accepts
a \lstinline{D} parameter that can be used to pass data between the visit calls,
and a return type \lstinline{R} that can be used to return a result from the 
visitor.
  \item \lstinline{FirVisitorVoid}: a visitor where each visit method accepts
only the element to visit and returns nothing. Under the hood is simply a 
\lstinline{FirVisitor<Unit, Nothing?>}.
\end{itemize}

Depending on the specific task assigned to the visitor, the developer will
choose one of these two types of visitors and extend it into a new class. Their
structure is summarized in the class diagram of
\cref{fig:fir-visitors-class-diagram}.

\begin{figure}
  \centering
  \includegraphics[width=.7\linewidth]{figures/fir-visitors.pdf}
  \caption{Summarized class diagram of the visitors inside the Kotlin 
  compiler}
  \label{fig:fir-visitors-class-diagram}
\end{figure}

\subsection{Pattern 3: unnecessary Yielding usage}

Having introduced the visitor pattern, we can now see it in practice having 
to deal with the next pattern, whose implementation in the checkers exploited
\ac{FIR} visitors. This pattern regard the \emph{unnecessary usage of the} 
\lstinline{yielding} \emph{contexts} in the Collektive DSL. 
%
The \lstinline{yielding} context is a specific feature inside the Collektive
\ac{DSL}, briefly introduced in \cref{sec:collektive-dsl}, that allows the
developer to call a construct of the \ac{DSL}, like \lstinline{exchange} for
instance, returning a value different from the result of the construct. To use
this feature, instead of using the actual construct, the variant with the
\lstinline{-ing} suffix is used, like \lstinline{exchanging} in this case, and
these variants are called called \textbf{yielding operations}. 
%
Essentially, a \lstinline{YieldingContext<Initial, Return>} is used inside
yielding operations to act on an \lstinline{Initial} value --- that can, for
example, be exchanged with neighbors --- but \textbf{return a different value of
type} \lstinline{Return} to the caller, without having to return the same value
as for the normal version.

In \cref{lst:p3-yielding-example} we can see an example of the usage of this
yielding context, taken from the Collektive documentation. 

\lstinputlisting[
  float, 
  language=Kotlin, 
  label={lst:p3-yielding-example},
  caption={Example of usage of the yielding context in the Collektive DSL, 
  taken from the Collektive documentation}
]{listings/P3YieldingExample.kt}

In the example provided, the \lstinline{exchanging} construct will still perform
like the normal \lstinline{exchange} construct, which sends the results from the
evaluation of the passed function to other devices and returns an object of type
\lstinline{Field<Int, Int>}, but the entire operation will return another object
of type \lstinline{Field<Int, String>}, resulted from the \lstinline{map} call
inside the yielding action.

As it may have been guessed, the usage of this feature can be redundant: nothing
prevents the developer from using this construct and return the same value as the
one been computed and sent to the other devices by the construct, resulting in a 
completely useless \lstinline{yielding} that could be safely substituted with the
normal version of the construct. 
%
An example of this Pattern is shown in \cref{lst:p3-example}.

\lstinputlisting[
  float, 
  language=Kotlin, 
  label={lst:p3-example},
  caption={Example of Pattern 3 detection in code, with an unecessary usage of a
  yielding context, this time with the \lstinline{sharing} construct}
]{listings/P3Example.kt}

\subsubsection{Formal description}

Given a Collektive construct that accepts a \lstinline{YieldingContext} as
parameter (e.g., \lstinline{exchanging}, \lstinline{evolving}, etc.), the
pattern is satisfied when the expression returned inside the anonymous function
passed as parameter to the \lstinline{yielding} call is \emph{equivalent} to the
one used as its \emph{receiver}, therefore resulting in a redundant usage of the
yielding context. 

\subsubsection{Design and Implementation}

As previously introduced, this Pattern's checker will be based on the Visitor
pattern. The checker can simply perform an initial check on the function name to
intercept a Collektive construct that accepts a \lstinline{YieldingContext},
based on a set of predefined, fully-qualified names. 
%
Once one of these constructs is found, the checker will delegate the inspection
of the anonymous function to the implemented visitor, that will traverse the
small part of the \ac{FIR} tree that is contained within the construct's
parameters (i.e., the anonymous function passed as parameter to the construct)
%
Finally, the visitor will return a boolean value used to report the successful
or unsuccessful detection of the pattern.

The implementation of the checker can be seen in \cref{lst:p3-checker}. The core
part is, of course, the visitor method call
\lstinline{containsUnnecessaryYielding}, whose implementation will be briefly
discussed in the following.

\lstinputlisting[
  float, 
  language=Kotlin, 
  label={lst:p3-checker},
  caption={Implementation of the Pattern 3 checker: \lstinline{UnnecessaryYielding}}
]{listings/P3Checker.kt}

\subsubsection{Visitor implementation}

To check if a function call corresponding to a construct with a yielding context
matches the pattern, we need to inspect the anonymous function that is passed as
parameter. When the visitor encounters a function call to the
\lstinline{yielding} operation, it will simply save the \textbf{explicit
receiver} of the call and then visit the anonymous function that is passed as
parameter to \lstinline{yielding}.
%
When visiting a \emph{return expression} of the anonymous function
passed to the yielding construct, the visitor will check if the expression is
\emph{equivalent} to the saved receiver of the yielding construct.
%
If so, the visitor will return \lstinline{true}, meaning that the pattern is
detected, otherwise it will return \lstinline{false}. 

A behavior like this can be easily implemented using the
\lstinline{FirVisitorVoid} class: the \cref{lst:p3-visitor} summarizes the visitor's
implementation.

\lstinputlisting[
  float, 
  language=Kotlin, 
  label={lst:p3-visitor},
  caption={Implementation of the Pattern 3 Visitor}
]{listings/P3Visitor.kt}

The function \lstinline{isStructurallyEquivalentTo} is a utility function that
uses Kotlin \emph{expression rendering} to check if two expressions are 
equivalent. For brevity purposes, its implementation is omitted.
%
The function \lstinline{containsUnnecessaryYielding} is also omitted, but its
implementation is a simple as calling the visiting methods and return a boolean
variable set during the exploration.

It appears quite clear how a visitor approach to this task is very effective and
limited in complexity compared to using the \ac{FIR} API on the elements
directly. However, this still represents a very simple case, often solved by
common static analysis checks for programming languages in similar situations
(e.g., same expression on both sides of an assignment). Let's see how to 
approach more complex cases with the visitor pattern.

\subsection{Pattern 4: unnecessary construct usage}

In order to prove the effectiveness of the visitor pattern in more complex cases,
we will now present a more complex pattern that can be detected using this
approach. This pattern regards the \emph{unnecessary usage of constructs} in the
Collektive DSL.

Kotlin compiler won't, by default, warn the developer when an anonymous function
that can have one or more parameters is created without using them in the body 
(and maybe passed to another function as a parameter). For example in the code
shown in the following snippet:

\begin{lstlisting}[language=Kotlin]
listOf(1, 2, 3).map { 5 }
\end{lstlisting}

The anonymous function passed to the \lstinline{map} function is not using the
\lstinline{it} parameter and, in general, this cannot always be interpreted as
an error because, depending on the function, the parameter might not be needed.
%
Some Collektive constructs, however, are designed to work with the parameters of
these anonymous functions, and the absence of these parameters could be a sign
of misinterpretation of the construct's behavior and intended usage as well as 
an unnecessary network exchange that results in a waste of resources.

Not all constructs behave in the same way: for example, the
\lstinline{neighboring} and \lstinline{neighboringViaExchange} constructs are
different from the rest. Since these two functions evaluate expressions in
neighbors devices, if the expression to evaluate is provided as an anonymous
function, this does not accept parameters. In this case, the construct usage is
considered unnecessary if the anonymous function has an \emph{empty return} ---
i.e., a return of type \lstinline{Unit}. 
%
An example of both the cases is shown in \cref{lst:p4-example}.

\lstinputlisting[
  float, 
  language=Kotlin, 
  label={lst:p4-example},
  caption={Examples of Pattern 4 detection in code, with both the cases described}
]{listings/P4Example.kt}

\subsubsection{Formal description}

Given a Collektive construct that accepts an anonymous function as parameter,
the pattern is satisfied when the body of the anonymous function does not use the
parameters that its signature accepts \emph{or} when the body of the anonymous
function returns \lstinline{Unit} in absence of accepted parameters (i.e., the
\lstinline{neighboring} and \lstinline{neighboringViaExchange} constructs).

\subsubsection{Design and Implementation}

Similarly to the previous pattern, the checker will be implemented with a very
simple logic, exploiting more complex visitors to inspect the pattern matching.
In this case, the checker will simply check if a function call is one of the 
Collektive constructs to check. If so, it will delegate the inspection on one
of two visitors, whether the construct is one of the constructs that accept
anonymous functions with parameters or not.

The implementation is summarized in \cref{lst:p4-checker}. As we can see, the
two visitors are \lstinline{EmptyReturnVisitor} and
\lstinline{ConstructCallVisitor}. The former is quite simple to implement (and,
therefore, will be explained without code): it behaves like a default
visitor, but when visiting a \lstinline{FirReturnExpression} element, it checks
if the return type is \lstinline{FirUnitExpression}. If so, the pattern is
detected.

\lstinputlisting[
  float, 
  language=Kotlin, 
  label={lst:p4-checker},
  caption={Implementation of the Pattern 4 checker: 
  \lstinline{UnnecessaryUseOfConstructs}}
]{listings/P4Checker.kt}

The second visitor is more complex: when checking for usages of parameters, we
\textbf{cannot simply check the return expression of the anonymous function} and
see if it uses the parameters, because the parameters can be used in other
expressions to create \textbf{dependent symbols} --- i.e., symbols that depend
on and can be used instead of the parameters in the return expression.
%
To solve this problem in a more simple way, the visitor will \textbf{not} check
if the dependent symbols are used in the return expression, therefore
implementing a \emph{symbol marking} system, but instead it will simply consider
the pattern as detected once it sees at least one usage of the parameter inside
the anonymous function body.  

The implementation of the \lstinline{ConstructCallVisitor} is shown in
\cref{lst:p4-visitor}. The variable \lstinline{found} will be set to \lstinline{true}
when the visitor encounters a usage of all parameters, meaning that the pattern is
detected when \lstinline{!found}.

\lstinputlisting[
  float, 
  language=Kotlin, 
  label={lst:p4-visitor},
  caption={Implementation of the Pattern 4 Visitor}
]{listings/P4Visitor.kt}

\section{4th approach: mixed approach}

\subsection{Pattern 5: improper Evolve construct usage}

\subsection{Pattern 6: complex aggregate operations in loops} \label{sec:p6}

%----------------------------------------------------------------------------------------
\chapter{Evaluation and Testing}
\label{chap:evaluation}
%----------------------------------------------------------------------------------------

When developing tools that need to inspect and modify source code, where
developers have freedom to write code in many ways and leveraging the full power of the 
used programming language, a solid and comprehensive testing strategy is crucial to 
ensure that the tool is working as expected and that it is able to capture all the
cases that it is supposed to capture.
%
If static analysis tools can be tedious to develop, due to the need to cover a
plethora of possible cases, often with a unified meta-level approach, testing
static analysis tools must be even more comprehensive and flexible than
development, trying not to fall into verbosity and repetition.

In this chapter we will present the testing strategy that was adopted to test the 
Collektive frontend plugin, starting from the initial approaches and moving into the 
development of an \emph{ad-hoc} testing framework named \emph{Subjekt}.

\section{Initial testing approaches}

The first approach taken was the most trivial: the tests were written using
\textbf{static Kotlin sources} as resource files, loading them in the test
suites written using the Kotlin test framework \textbf{Kotest}. The first
problem that arose regarded the \emph{compilation process and the check for
resulting diagnostics}. To test the checkers, in fact, the code must be compiled
programmatically also providing the plugin to the compiler, and then collect all
the produced messages and compare them with the expected ones.
%
Fortunately, a library already exists for this purpose: \textbf{Kotlin Compile
Testing} by Thilo Schuchort (tschuchortdev on GitHub)\footnote{The actual
library that was used is a fork of the original one, maintained by Zac Sweers
(ZacSweers on GitHub). The switch to the fork was necessary due to some issues
encountered during the building process of the project with Kotlin 2.0 which,
at the time of writing, is still not supported by the original library. More
information is available here:
\url{https://github.com/tschuchortdev/kotlin-compile-testing/issues/411}}.
This library provides a simple way to create objects that represent Kotlin source
files by passing a string source code, and then compile them giving a list of
compiler plugins or annotation processors that should be used during the
compilation process. All of this is done \emph{inheriting the class path} of
where the compilation process is running, so the Collektive plugin can be added
without any problem also adding the corresponding library. Finally, the
diagnostics produced are contained inside the
\lstinline{KotlinCompilation.Result} object as a single string that can be
easily split and filtered to get the relevant messages and compare them with the
expected ones.

With this library, it is now possible to write tests simply by reading the
resource files that contain the source code that should result or not in a
pattern being detected. These testing cases should be the same that were
extracted from the codebase during the development of the checkers. An example
of a test suite using this method can be seen in \cref{lst:initial-test-suite}.
%
\lstinputlisting[
  float, 
  language=Kotlin, 
  label={lst:initial-test-suite},
  caption={One of the first developed test for the Pattern 2 checker. It uses
  static resource files to load the source code to be compiled and checked}
]{listings/TestingExample1.kt}
%
As we can see, the testing process is quite simple and straightforward:
\lstinline{TestAggregateInLoop.kt} is a source file contained in the resource
folder and contains a simple source code with an aggregate construct used inside
a loop, which is the case that should be detected by the checker for Pattern 2.

It appears quite clear however, how this approach soon proves to be un-scalable
and especially very repetitive, as for the same pattern to be tested there are
many possible source codes in which the checker might have to operate (and
therefore cases to be tested) and where the difference with the previous ones
is, in most cases, a line or two of code. 
%
Following this method, the test suite would have needed many static files, each
very similar to some others, and the Kotest spec would have been very verbose,
even trying to minimize the repeated parts through the use of Kotest utility
functions like \lstinline{forAll}.

\section{Avoiding repetitions through template files}

One of the first solutions to the problem of repetition when dealing with
textual files very similar to each other is to use \textbf{template files}. The
approach is quite simple: for each pattern to test, collapse all the testing
cases that are similar to each other into a single file containing placeholders
for the parts that change. 

\subsection{Initial template system}

\note{TODO: describe the initial template system}

A template file structured like this is shown in
\cref{lst:template-file}.
%
\lstinputlisting[
  float, 
  language=Kotlin, 
  label={lst:template-file},
  caption={A example of the templates created for testing multiple cases at 
  once}
]{listings/TestingTemplateFile.kt}
%
Then, instead of compiling the string obtained from the
resource file directly, replace the placeholders cycling through a list of
possible, hard-coded configurations in the test suite and for each perform the
correct check on the diagnostics produced.
%
In addition, we extract a small utility in order to make the process of reading
resources, compiling and comparing messages more concise and readable. The
result looks similar to the one shown in \cref{lst:template-test-suite}. Only a
relevant part of the test suite is shown, the rest is omitted for brevity.

\lstinputlisting[
  float, 
  language=Kotlin, 
  label={lst:template-test-suite},
  caption={Part of the test for Pattern 2 revised using template files}
]{listings/TestingExample2.kt}

\subsection{Templates flexibility and limitations}

Even though this approach gained some flexibility and reduced repetitions on the
template files side, it still requires a lot of boilerplate code to be written
between each test case. Another step is necessary to make the testing process
even more concise: integrating the created utility with a string interpolation
mechanism that allows configurable parts without repeated and obscure code for
formatting. 
%
This allows to write templates that are more clear and readable, because instead
of writing placeholders with \lstinline{%s} we can write variables with names 
like \lstinline{%(nameOfTemplateVariable)} and then pass a configurable map 
through the test code that will replace these variables with the correct values.
All of these utilities, contained in the singleton object \lstinline{CompileUtils},
permit writing tests like the ones shown in \cref{lst:template-revised-suite}.
%
\lstinputlisting[
  float, 
  language=Kotlin, 
  label={lst:template-revised-suite},
  caption={\Cref{lst:template-test-suite} revised using the new utilities}
]{listings/TestingExample3.kt}
%
It is easy to see how the test not only gained in readability, but it is much
more scalable and can be used to test many more cases adding only a few lines of
code. 

\section{Code generation: a small DSL leveraging Kotlin Poet}

\subsection{General structure and usage}

\subsection{Considerations}

\section{Custom testing framework: Subjekt}

\subsection{Main ideas behind the framework}

\subsection{Core structure of Subjekt}

\subsection{Final usage inside tests}

%----------------------------------------------------------------------------------------
\chapter{Conclusions}
\label{chap:conclusion}
%----------------------------------------------------------------------------------------

\section{Opportunities of Compiler plugins}

\section{Approaching meta-level analysis}

\section{Future works}

%----------------------------------------------------------------------------------------
% BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\backmatter

\nocite{*} % Remove this as soon as you have the first citation

\bibliographystyle{alpha}
\bibliography{bibliography}

\begin{acknowledgements} % this is optional
Optional. Max 1 page.
\end{acknowledgements}

\end{document}
