\documentclass[12pt,a4paper,openright,twoside]{book}
\usepackage[utf8]{inputenc}
\usepackage{disi-thesis}
\usepackage{code-lstlistings}
\usepackage{notes}
\usepackage{shortcuts}
\usepackage{acronym}

\school{\unibo}
\programme{Corso di Laurea Magistrale in Ingegneria e Scienze Informatiche}
\title{Development Process of a Compiler Plugin for Aggregate Computing}
\author{Francesco Magnani}
\date{\today}
\subject{Software Process Engineering}
\supervisor{Danilo Pianini}
\cosupervisor{Nicolas Farabegoli}
\morecosupervisor{Angela Cortecchia}
\session{III}
\academicyear{2023-2024}

% Definition of acronyms
\acrodef{IoT}{Internet of Thing}
\acrodef{vm}[VM]{Virtual Machine}
\acrodef{IDE}[IDE]{Integrated Development Environment}
\acrodef{CI}[CI]{Continuous Integration}
\acrodef{AST}[AST]{Abstract Syntax Tree}
\acrodef{KAPT}{Kotlin Annotation Processing Tool}
\acrodef{KSP}{Kotlin Symbol Processing}
\acrodef{IR}{Intermediate Representation}
\acrodef{DSL}{Domain Specific Language}
\acrodef{IR}{Intermediate Representation}
\acrodef{FIR}{Frontend Intermediate Representation}
\acrodef{PSI}{Programming Structure Interface}


\mainlinespacing{1.241} % line spacing in mainmatter, comment to default (1)

\begin{document}

\frontmatter\frontispiece

\begin{abstract}	
Max 2000 characters, strict.
\end{abstract}

\begin{dedication} % this is optional
Optional. Max a few lines.
\end{dedication}

%----------------------------------------------------------------------------------------
\tableofcontents   
\listoffigures     % (optional) comment if empty
\lstlistoflistings % (optional) comment if empty
%----------------------------------------------------------------------------------------

\mainmatter

%----------------------------------------------------------------------------------------
\chapter{Introduction}
\label{chap:introduction}
%----------------------------------------------------------------------------------------

Analyzing characteristics of the source code without necessarily building and
executing it (i.e., static analysis) is a process that has been studied and
implemented in various forms during the last decades. Various tools have been
developed to perform such a task, each with its own strengths and weaknesses.
%
The need for such a tool is often evident in the software development process,
where ensuring the \textbf{quality} and \textbf{robustness} of software systems
is a critical concern. Using code quality analysis techniques is also a powerful
mean to avoid situations of ``technical debt''
\cite{DBLP:conf/sigsoft/ErnstBONG15}, that targets the system quality in
maintenance and evolution.

As the system grows in complexity, so does the variety of errors and
vulnerabilities that can be detected through tools (e.g., concurrency management
issues, error handling, etc.). At the same time, adhering to coding standards
helps to avoid both trivial and non-trivial errors from the outset.
Additionally, these are among the easiest types of tools to integrate, often
already included within \acp{IDE}.
%
The effectiveness of static analysis tools has been the subject of various
studies,  \cite{DBLP:journals/jss/LenarduzziPSLP23} evaluating their detection
capabilities, agreement, and precision. Some of these studies revealed a low
degree of agreement among the tools and highlighted the need for a better
understanding of their actual capabilities. More advance tools have been used
also to rewrite code and help with the development of very complex systems,
like \emph{Coccinelle} for the Linux kernel
\cite{DBLP:conf/eurosys/PadioleauLHM08}\cite{DBLP:conf/usenix/LawallM18}.
%
More over, in the last years, the static analysis tools have become more popular
and easier to use, becoming protagonists of many \ac{CI} pipelines
\cite{DBLP:conf/msr/ZampettiSOCP17} that automatically performs checks on the
entire source code, embracing change and evolution of the software without
making it a threat \cite{DBLP:books/daglib/0015650}, backed by a solid safety
net.

If looked from another perspective, automated static analysis tools and
compilers share significant similarities in their operations. Both perform
thorough examinations of source code without executing it, aiming to identify
errors, enforce coding standards, and optimize performance. Most of what
compilers do on the static analysis side is to facilitate code optimization and
error detection during the compilation process. In fact, a compiler can be
viewed as a form of static analysis tool, as it analyzes code to generate
executable programs and associated debugging information
\cite{DBLP:journals/queue/Thomson21}.
%
Specialized static analysis tools, on the other hand, extend beyond the
capabilities of standard compilers by offering additional functionalities and
broader diagnostic capabilities. They offer a broader range of diagnostic rules,
enabling the detection of specific and uncommon bugs that compilers might
overlook.
%
What if, however, the compiler were extended beyond its core functionality,
incorporating specialized capabilities that go beyond the scope of a
general-purpose environment? These extensions could be designed to address the
unique requirements of a specific project or domain. This is precisely where
\textbf{compiler plugins} come into play.

\section{Definition and Purpose of Compiler Plugins}

Compiler plugins are dynamic modules that interact with the compiler during its
various phases, enabling the introduction of new functionalities or the
modification of existing behaviors. They serve as intermediaries that can
inspect, modify, or enhance the compilation process, providing developers with
the flexibility to implement domain-specific checks, optimizations, or
transformations, all without altering the compiler's core architecture. 
%
For instance, in the context of the GNU Compiler Collection
(GCC), plugins allow for the addition of new features without necessitating
modifications to the compiler itself (mostly, again, for optimization purposes).

\subsection{Types of Compiler Plugins}

Compiler plugins can be broadly categorized based on the phase of compilation they target:

\begin{itemize}
  \item \textbf{Frontend Plugins}: These plugins operate during the initial
  stages of compilation, focusing on tasks such as syntax analysis, semantic
  analysis, and \ac{IR} generation. They are useful also for
  implementing custom syntax extensions, enforcing coding standards, or
  performing static code analyzes. For example, in the \emph{Rust} programming
  language, compiler plugins can introduce new syntax extensions and lint
  checks. 

  \item \textbf{Backend Plugins}: Functioning in the latter stages of
  compilation, backend plugins are concerned with code optimization, machine
  code generation, and platform-specific adjustments. They can be utilized to
  implement custom optimizations, support additional hardware
  architectures and more.
\end{itemize}

\subsection{Compiler Plugins in Kotlin}

Kotlin, a statically typed programming language developed by JetBrains, offers
robust support for compiler plugins, allowing developers to highly customize the
compilation process to their specific needs. 
%
Kotlin's compiler architecture facilitates the creation of plugins that can
modify or extend its behavior during compilation. For example, the
\emph{all-open} compiler plugin in Kotlin allows classes annotated with a
specific annotation to be open without the explicit \lstinline{open} keyword,
adapting Kotlin to the requirements of frameworks that need classes to be
open. 

When guiding developers towards the creation of compiler plugins, JetBrains
compares them to \textbf{Annotation Processors}
\cite{JetBrains:KotlinCompilerPlugin}. Annotation processors are a powerful
feature in many modern programming languages, including Java and Kotlin, that
allow developers to generate code, validate code, and perform various
compile-time checks based on \textbf{annotations} present in the source code.
%
In Java, annotation processors are part of the Java Compiler API and can be used
to generate additional source files, validate the correctness of the code, and
even modify the \ac{AST} of the code being compiled. They are
commonly used in frameworks and libraries to reduce boilerplate code and enforce
coding standards.
%
Kotlin also supports annotation processors through the \ac{KAPT} — which is, in
fact, a compiler plugin itself — that allows Kotlin code to interoperate with
Java annotation processors and, more recently, the \ac{KSP}, another compiler
plugin introduced as ``an API that you can use to develop lightweight compiler
plugins''. The former enables developers to leverage existing Java annotation
processors in their Kotlin projects and the latter, on the other hand, provides
a more efficient and Kotlin-specific way to generate code at compile time,
offering a new approach that is much more integrated with Kotlin symbols.

\subsection{Compiler Plugins advantages}

Compiler plugins and Annotation Processors, however, have some very distinct
functionalities. While Annotation Processors are limited to generating source
code and performing checks based on annotations, compiler plugins can exploit a
very powerful API that can create and modify byte-code, elements inside the
\ac{IR} and more, allowing the developers to solve a whole new class of
meta-programming problems. Of course, Annotation Processors are typically easier
to write and maintain than compiler plugins, but this extra cost can be worth in
several cases, for example in the scenario that will be presented in this
thesis.

\section{Development of a Frontend Compiler Plugins}

At the time of writing, the development of frontend compiler plugins in Kotlin is
still a relatively less explored area compared to the backend ones. Because of the 
very little documentation and examples available, the development of frontend 
compiler plugins can be a challenging task, that quite often requires inspecting 
the Kotlin compiler source code to understand how to interact with it.
%
Frontend compiler plugins can be implemented using \textit{Extensions} to the
Kotlin compiler, a topic that will be covered more in detail in the following
chapters. This thesis presents the development process of a frontend compiler
plugin designed to build upon an existing \textit{backend} plugin. The primary
purpose of this frontend plugin is to perform static checks on source code,
ensuring compliance with specific rules related to the functionality of the
pre-existing backend plugin.

\subsection{The importance of Build Tools}

Before proceeding with the explanation of the main structure of this thesis, it
is important to mention the role of build tools in the development process of a
compiler plugin. Without a seamless integration of the compiler plugin during
the compilation process (and, as it will be shown, the testing process as well), the
development of a compiler plugin would be cumbersome and error-prone. 
%
Needless to say, a well-configured build tool is a fundamental requirement in
this context. Within the Kotlin ecosystem — on which this work focuses — the most
widely used build tool is \emph{Gradle}, and it will serve as the foundation for
the implementations discussed in this thesis.

\paragraph{Structure of the Thesis}

This thesis follows the development process of a frontend compiler plugin from
its initial steps, addressing the key challenges encountered as well as the
solutions proposed during the research and implementation phases. The next
chapter, \cref{chap:background}, provides an overview of the ongoing project for
which this frontend plugin is being developed, alongside the technical
background necessary to understand how a plugin can interact with the Kotlin
compiler.
%
\Cref{chap:contribution} delves into the core development process of the plugin,
discussing the design decisions, alternative approaches considered, and the
final implementation of the proposed \textbf{checkers}. Subsequently,
\cref{chap:evaluation} evaluates the plugin’s behavior, with a particular focus
on the testing methodology adopted, including the integration of a custom
testing framework named \emph{Subjekt}, developed specifically for this purpose.
%
Lastly, \cref{chap:conclusion} summarizes the primary contributions of this
thesis and outlines potential directions for future work, building on the
results and insights gained throughout this research.

%----------------------------------------------------------------------------------------
\chapter{Background: the Collektive case}
\label{chap:background}
%----------------------------------------------------------------------------------------

As already stated, the development of the frontend compiler plugin presented in
this thesis will be built on top of an existing backend plugin. The backend
plugin is part of a larger project named \emph{Collektive}, a Kotlin
multiplatform framework that provides a \ac{DSL} for the \textbf{Aggregate
Computing} \cite{Beal2015} paradigm. This chapter will provide an overview of
the concepts behind Collektive as well as the feature behind its backend plugin
and Kotlin compiler plugins development in general.

\section{Aggregate Computing}

The Aggregate computing paradigm is a novel approach to ``design, create and
maintain'' \cite{Beal2015} complex distributed systems, particularly in the 
context of the Internet of Things (IoT). The paradigm shifts the focus from
individual devices to regions of devices, abstracting away the details of their
number, position, and behavior. This abstraction enables developers to reason
about distributed systems in terms of \emph{collective} operations over spatial and
temporal fields, rather than device-to-device interactions. 

The foundation of aggregate programming is built on \textbf{field calculus}
\cite{Beal2014TowardsAU}, a set of constructs designed for spatial computing.
These constructs enable the implementation of robust coordination mechanisms,
such as self-stabilizing and scalable operations \cite{Viroli2018}
\cite{DBLP:journals/jlap/ViroliBDACP19}, which adapt dynamically to changes in
the environment. Applications of aggregate programming are particularly
impactful in large-scale scenarios, such as crowd management during public
events, where distributed devices coordinate to provide services like crowd
density estimation, dispersal advice, and navigation support.

Aggregate computing formalism has been proposed in various ways, introducing
syntaxes and semantics to support distributed, collective behaviors in dynamic
systems. Field calculus, as a foundational model, supports aggregate computing
by enabling global-level manipulation of computational fields through a
minimalistic syntax. 
%
Tools like Protelis \cite{DBLP:conf/saso/PianiniBV17}, ScaFi
\cite{DBLP:conf/ecoop/CasadeiV16} and the recently proposed eXchange Calculus
(XC) \cite{DBLP:journals/jss/AudritoCDSV24} extend field calculus principles,
providing programming frameworks and language constructs to bridge the gap
between theoretical models and practical implementations.

Managing the programming line of action for these kinds of systems is not
trivial, but several approaches have already been proposed, including static and
dynamic checks that can spot subtle bugs and vulnerabilities in the code, by
focusing on concepts like \textbf{neighborhood interactions} and \textbf{domain
alignment} \cite{DBLP:conf/saso/AudritoDVC16}.

\subsection{Main aspects}

In Aggregate Computing, the main model of the system consists of a \emph{network of 
intercommunicating devices}, which can be \emph{close} to one another therefore 
introducing a concept of \textbf{neighborhood}. 
%
Each device is equipped with sensors and actuators, enabling interaction with an
environment, and can communicate with other devices through a
\emph{message-passing} system. 

A key aspect of Aggregate Computing regards the execution model, which is based
on a local program identical for all devices. The system is governed by a
continuously executed loop that makes the devices \emph{receive messages},
\emph{produce a result through their internal behavior} and finally \emph{send
values to neighbors}. 
%
This simple structure allows for system to show self-organizing behaviors
emerging from the \emph{collective} of devices, and contemplates even complex
interactions of the devices with the environment (e.g. creation of new devices).

Finally, as already said, the \emph{field calculus} model can be implemented
through an ad-hoc API and syntax to perform operations on a \emph{computational
field} (i.e., a mapping from device locations to values in our example)
manipulating it over time, defining interactions between devices and finally
creating the emergent behavior.
%
After the previous tools already cited, it is now necessary to
introduce the Aggregate Computing framework that will be the target of the
frontend plugin in this thesis: \emph{Collektive}.

\section{Collektive: an Aggregate Computing Framework}

\emph{Collektive} is a modern Aggregate Computing framework developed in Kotlin 
that allows developers to easily write Aggregate Computing programs through a 
flexible \emph{internal} DSL. The framework is designed to be multiplatform,
targeting both JVM and JavaScript platforms, as well as native ones. 
%
Compared to some other existing Aggregate Computing frameworks, Collektive
offers a modern and idiomatic approach to writing Aggregate Computing programs,
with a static type system (as it is internal to Kotlin) and few, expressive
constructs like \lstinline{neighboring} and \lstinline{exchange}, which can be
used to implement a broad variety of interactions between devices (and also
Aggregate Computing patterns).

The project is organized in modules, with the main one being the \lstinline{dsl}
and \lstinline{compiler-plugin}, and is tested using the Alchemist simulator
\cite{DBLP:journals/jos/PianiniMV13}. 

\subsection{Collektive DSL: main concepts}

In order to understand how to work on Collektive programs, we first need to
understand its main usage. Collektive DSL is centered around the
\lstinline{aggregate} function, which is the entry point for the Aggregate
Computing program. The function uses a local ID to identify the device on which
is executed on and then accepts a function type with receiver
\lstinline{compute: Aggregate<ID>.() -> R} that performs the aggregate
computation using the \lstinline{Aggregate} interface members.
%
The \lstinline{Aggregate} interface provides several functions that represent
the main constructs of Collektive's implementation of the Aggregate Computing
paradigm. The most important ones are:
\begin{itemize}
  \item \lstinline{neighboring}: a function that observes expressions on
  neighbors, returning the related field;
  \item \lstinline{exchange}: a function that, taken from the documentation of
  Collektive, ``manages the computation of values between neighbors in a
  specific context''. In practice, it can be used to compute a new field of 
  values starting from initial values of neighbors;
  \item \lstinline{evolve}: updates an initial value iteratively computing an
  expression at each device;
  \item \lstinline{share}: performs a field computation starting from neighbors'
  values, reducing them to a single value and then sharing it with neighbors. 
  It can be used to actuate a ``space-time evolution'' on the field.
\end{itemize}

Some of these constructs is present also in a variant that allows to return a
different type of value from the initial one; the name of the variant is 
the same of the original function but with the suffix \lstinline{-ing} (e.g., 
\lstinline{exchanging}, \lstinline{evolving} etc.).

Just by using these four constructs, already complex collective behaviors can
be achieved. Besides that, the Collektive DSL hides several operations through
the already mentioned compiler plugin. 

\subsection{Collektive Compiler Plugin}

As previously introduced, Collektive provides an already integrated
\textbf{backend compiler plugin}. This plugin is responsible for managing the
\emph{alignment of aggregate computation}, a very important concept when dealing
with Aggregate computing programs, faced in several studies
\cite{DBLP:conf/forte/DamianiVPB15} \cite{DBLP:conf/saso/AudritoDVC16}.
%
Essentially, the domain alignment is a necessary step to ensure that when a
device computes a value that depends on neighboring devices (e.g., through the
\lstinline{neighboring} construct, which retrieves values from neighbors), those
neighboring devices have computed the same expression in the same evaluation
round. This guarantees that shared computations remain consistent across
devices: it's necessary in order to \textbf{maintain consistency between field
values} and \text{prevent information leakage} because ``without it, information
may leak unexpectedly between devices that are evaluating different functions,
or may be “blocked” from passing between devices evaluating the same function''
\cite{DBLP:conf/forte/DamianiVPB15}.

Collektive \emph{backend} compiler plugins works by analyzing call sites and
function definitions in the code, intercepting the ones that involve aggregate
computation and that should, therefore, be ``aligned''. Essentially, this is
done by looking at \lstinline{Aggregate} interface usage in functions,
especially when used as receiver, and visiting their declarations, wrapping
aggregate constructs usages with special \lstinline{align} and
\lstinline{dealign} functions that perform domain alignment under the hood.
%
As we will see, this is not always automatic or safe in certain situations 
(e.g., loops), and the frontend extension will take care of these extra 
checks. 

But how do Kotlin compiler plugins work in general? Before diving into the 
core development of the frontend plugin, it is necessary to understand their
main structure and how they interact with the Kotlin compiler.

\section{Kotlin Compiler Plugins} 

Since Kotlin is a multiplatform language, the same source code can be compiled
into low-level code specific to different targets, such as the JVM, JavaScript,
and native platforms. In order to work with different targets, the Kotlin
compiler architecture is divided into two sub-parts: the \emph{frontend} and the
\emph{backend}\footnote{The following explanation is greatly
inspired from the work of \cite{moskala2023}, which provides a more
comprehensive overview of the Kotlin compiler plugins architecture.}.
%
The frontend is independent of the target, and for this reason its output can be 
reused when targeting different platforms. Currently, this part of the compiler 
is being migrated to the new K2 version, which is supposed to be much more 
efficient than the previous K1 version.
%
The backend, on the other hand, is mostly specific to the target platform, and
uses the output of the frontend to generate the final code. The backends for
JVM, JS, Native, and WASM share some parts, that will be analyzed later. The
general structure is summarized in \cref{fig:kotlin-compiler-architecture}.

\begin{figure}
  \centering
  \includegraphics[width=.8\linewidth]{figures/kotlin-compiler-architecture.pdf}
  \caption{Kotlin compiler architecture. Adapted from the one at \cite{moskala2023}}
  \label{fig:kotlin-compiler-architecture}
\end{figure}

The frontend is also responsible for communicating with IDEs and build tools, 
providing APIs to present errors, warnings, code completions and so on. 
%
To make this architecture modular, the Kotlin compiler provides a set of 
\ac{IR}, that the various steps of the workflow
can use to process the preceding steps' output. Both the frontend and the
backend creates this data structure, although they are very different. The 
backend's one is created starting from the output \ac{IR} of the frontend,
while the frontend's one is created from the Kotlin source code. 
The general workflow is summarized in \cref{fig:kotlin-compiler-workflow}.

\begin{figure}
  \centering
  \includegraphics[width=.8\linewidth]{figures/kotlin-compiler-workflow.pdf}
  \caption{Kotlin compiler general workflow. Adapted and extended from the one
  at \cite{moskala2023}}
  \label{fig:kotlin-compiler-workflow}
\end{figure}

Before the so-called K2 frontend the compiler's frontend worked differently,
building the \ac{PSI}, a syntactic model of the parsed source code, and the
\lstinline{BindingContext}, that holds semantic information such as types and
symbol bindings (represented in \cref{fig:kotlin-compiler-workflow} as a whole). 
%
The new K2 frontend, on the other hand, builds the \ac{FIR}, a more powerful and
complete representation of Kotlin parsed code, capable of offloading some of the
work that was previously done by the backend and enabling powerful optimizations
and caching mechanisms. The raw \ac{PSI} is still being produced, but it is now
transformed into the \emph{raw FIR}, which again transforms in different stages,
filling the tree with semantic information. Finally, the resolved tree is passed
to the backend, which takes care of the platform-specific (backend) \ac{IR}, used
to generate the final code.

As we can see in \cref{fig:kotlin-compiler-workflow}, there is still another
process that hasn't been mentioned yet: the \emph{checkers}. During their stage
of execution, the checkers can inspect the \ac{FIR} and reports different
diagnostics. If some of them is considered ``critical'', in the sense that the
compilation should not proceed (e.g., a type error), the compilation is stopped,
and the backend is not executed: otherwise, the final \ac{IR} is passed to it.
%
The role of the checkers is particularly important in the context of this
thesis, and the way it's possible to interact with them is by means of
\textbf{extensions}.

\subsection{Extensions}

Compiler extensions are mechanisms that allow developers to modify various
phases of the compilation process, either by analyzing and transforming code at
the \ac{FIR} level or by modifying the backend \ac{IR}. These extensions
enable advanced features such as additional type checks, automated code
generation, and optimizations, empowering Kotlin’s extensibility.
%
Like with compiler stages, extensions can be either frontend or backend as well.
While frontend extensions impact code analysis, syntax resolution, and IDE
support, backend extensions act right after the \emph{IR generation +
optimization} phase seen in \cref{fig:kotlin-compiler-workflow}. This
distinction makes frontend extensions more suitable for language-level changes
and linting rules, whereas backend extensions are primarily used for performance
optimizations and bytecode transformations.

K2 introduces multiple frontend extensions, all following the
\lstinline{Fir[Name]Extension} convention. In relation to what was previously
discussed, the \lstinline{FirAdditionalCheckersExtension} is particularly
important in this context, since it's a frontend extension that allows
developers to register \emph{additional checkers} to run during compilation.
These checkers can enforce custom coding rules, report warnings, or even prevent
compilation by issuing errors. More over, errors and warnings generated by this
extension appear in IDEs like IntelliJ IDEA, improving real-time code feedback.
This extension is used by Kotlin plugins like \emph{Kotlin Serialization},
which ensures that serialization-related constraints are properly followed, and
\emph{Arrow Meta}, which enforces functional programming best practices.

The extension mechanism is also used in the backend, where backend plugins can
intervene, but the extension that needs to be used in only one:
\lstinline{IrGenerationExtension}. This extension is invoked after the \ac{FIR}
phase has completed and the \ac{IR} has been generated. It allows modifications
to the IR tree before it is used for bytecode generation. Because IR sits
between the frontend and the platform-specific backend, changes made here can
impact the generated machine code without altering the high-level source
representation. 
%
A key limitation of this extension is that it does not influence code analysis
in IntelliJ IDEA. Any method or class injected via this extension will not be
recognized by the IDE, meaning that developers cannot call it directly unless
they use reflection. This extension is widely used in performance-critical
applications. For instance, \emph{Jetpack Compose} leverages it to transform
composable functions into an optimized internal representation. Similarly,
\emph{Kotlin Serialization} uses it to generate serialization methods dynamically,
ensuring they are both efficient and lightweight. However, modifying IR directly
can introduce breaking changes if not handled carefully. Since IR
transformations occur at a low level, even minor alterations can lead to
unintended consequences, making this extension a powerful but complex tool.

Collektive backend plugin uses the \lstinline{IrGenerationExtension} to perform 
the operations we already discussed. To register an extension, the developer needs
to declare a class that extends \lstinline{CompilerPluginRegistrar}. Inside that,
the developer can register the extension using the \lstinline{registerExtension}
method that depends on the type of extension that is being registered. 
%
The core login of the plugin is then implemented in the extension itself. In the
case of the Collektive backend plugin, the class structure is summarized 
in \cref{fig:extensions-class-diagram}.

\begin{figure}
  \centering
  \includegraphics[width=.9\linewidth]{figures/extensions-class-diagram.pdf}
  \caption{Class diagram representing the top-level structure of the Collektive 
  backend compiler plugin. Note: the frontend plugin is not included yet.}
  \label{fig:extensions-class-diagram}
\end{figure}

Finally, to include the plugin, a \lstinline{META-INF/services} file is needed,
where the fully qualified name of the \lstinline{AlignmentComponentRegistrar} is
written, and the plugin is wrapped in a Gradle plugin that can be applied to the
project build. After the application of the frontend plugin also, the structure
of the project's components will be the one described in
\cref{fig:collektive-components-diagram}.

\begin{figure}
  \centering
  \includegraphics[width=.6\linewidth]{figures/collektive-components.pdf}
  \caption{Components of the Collektive project after the application of the frontend plugin}
  \label{fig:collektive-components-diagram}
\end{figure}

\section{Motivations}

Before diving into the development of the frontend plugin, let's briefly
summarize the motivations behind the project. As previously mentioned, Aggregate
computing is still a relatively new paradigm, and some studies
\cite{DBLP:conf/saso/AudritoDVC16} already pointed out subtle bugs that can
occur when using some of its possible implementations. 
%
Collektive is not an exception and, in this particular case, developing a
frontend compiler plugin is motivated by three main reasons:
\begin{enumerate}
  \item \textbf{The backend plugin}: since a backend plugin is already present
  and necessary for the correct functioning of the framework, the frontend
  plugin can just be built on top of it, extending its functionalities and
  ensuring that the user's operations are safe and within the scope of
  Collektive DSL correct usage principles.

  \item \textbf{The presence of an internal DSL}: when building a DSL, a
  developer must choose to make it \emph{internal} to a host language, like
  Kotlin, or \emph{external}, like Protelis \cite{DBLP:conf/saso/PianiniBV17},
  and so take care of the parsing and the compilation process in general.
  %
  Typically, internal DSLs are easier to use and to develop, but they can also
  be less flexible and more error-prone, because the developer does not have
  full control over the parsing process and therefore cannot always enforce the
  constraints that the DSL should have. This is the case with Collektive, and
  the frontend plugin can help in this regard.
  
  \item \textbf{The integration with the other development tools}: developing a
  frontend plugin integrates very well with the other tools that are already 
  used in most projects, for example:

  \begin{itemize}
    \item \textbf{IDE}: since the IntelliJ IDEA \ac{IDE} is the most indicated
    for Kotlin development and it has a very good support for the Kotlin
    compiler, the frontend plugin can leverage this support to provide real-time
    feedback for diagnostics and errors it produces, without the need of
    developing specific \ac{IDE} plugins.

    \item \textbf{Build tools}: the Gradle plugin that is used to apply the
    compiler plugin is pretty simple and needs almost zero modifications when
    extending the compiler plugin, differently from what would happen in the
    case of Gradle plugins wrapping external tools (e.g. \emph{ktlint}). This
    means that it can be easily maintained and integrated into other Gradle
    projects with low effort.

    \item \textbf{Rest of the pipeline}: the frontend plugin can be developed 
    very closely to the rest of the Collektive framework through submodules,
    adding dependencies between subprojects in a very clean and maintainable
    way.
  \end{itemize}
\end{enumerate}

Considering these motivations, we can now proceed with the actual development in
the next chapter.

%----------------------------------------------------------------------------------------
\chapter{Frontend plugin}
\label{chap:contribution}
%----------------------------------------------------------------------------------------

In this chapter we will present the development process that lead to the creation of 
the frontend plugin inside the Collektive project.

\paragraph{Structure of this chapter}

In this chapter, we will first present the general workflow of the plugin's
checkers, then we will proceed presenting several \textbf{patterns} that were
detected observing the Collektive DSL codebase. These patterns represent bad or
inappropriate use cases of the Collektive DSL that are \textbf{not} captured by 
default. 
%
The way these patterns will be presented does not follow the chronological order 
of their development, but rather an order with an increasing level of complexity.
For this purpose, the patterns will be categorized into four main groups, 
reflecting the design decisions that were taken to approach the related problems,
highlighting the pros and cons of each approach. 

\textbf{Note}: the features of the Kotlin compiler explained and shown in
this chapter are \emph{experimental}, and therefore subject to possible instability
and frequent changes. The code snippets and examples provided are based on the
current state of the Kotlin compiler at the time of writing. 

\section{Workflow}

After defining the \lstinline{CollektiveFrontendExtensionRegistrar} class that
wraps the frontend plugin, already introduce in \cref{chap:background}, the
first step is to register the actual extensions to the compiler using the
\lstinline{configurePlugin} method. For this plugin, the extension that will
be used is the already cited \lstinline{FirAdditionalCheckersExtension}, which
allows to register additional checkers to run during compilation.
%
This extension can contain an arbitrary number of checkers, each of them
assigned to different elements of the \ac{FIR} tree (e.g., expressions,
declarations etc.). All the checkers that will be presented are added 
through this extension. The final class structure is summarized in
\cref{fig:frontend-class-diagram}.

\begin{figure}
  \centering
  \includegraphics[width=.8\linewidth]{figures/frontend-class-diagram.pdf}
  \caption{Class diagram summarizing the structure of the frontend plugin}
  \label{fig:frontend-class-diagram}
\end{figure}

From this point on, the workflow of the plugin development will be as follows:

\begin{enumerate}
  \item \textbf{Pattern detection}: when a new pattern that needs to be captured
  by the plugin is detected, several examples of Collektive programs expecting a
  positive or negative  diagnostic are written (i.e., respectively, cases in
  which the diagnostic should be reported and cases where it shouldn't).
  \item \textbf{Test arrangement}: the examples previously written are used to 
  create a test suite that will be used as certification of the pattern's correct
  capture. 
  \item \textbf{Checker creation and implementation}: a new checker is created and
  added to the frontend extension, implementing the logic that will detect and 
  correctly report positives of the pattern's usages.
  \item \textbf{Adjustments and identification of corner cases (optional)}: during
  the checker's development some corner cases may arise that need to be handled 
  by this checker and added as regression tests. In this case, after the corner 
  cases are correctly identified, this goes back to step 2, working on the already
  created checker.
\end{enumerate}

In this chapter we will present these steps for each pattern except for the step
2, that will be covered in \cref{chap:evaluation}. 

\textbf{Note}: the code snippets that will be presented in the following
sections, if not appropriately specified in the code, present portions of code
that are \textbf{inside an Aggregate block}, therefore contained either in a
function that has the \lstinline{Aggregate} interface as receiver or in a 
\lstinline{aggregate} entry point block, since these are the places where 
the Collektive DSL can be used to specify aggregate behaviors. This has been
done for the sake of brevity and clarity.

\section{1st approach: direct Kotlin API usage}

The API that Kotlin provides to interact with the \ac{FIR} is sufficiently
powerful to be used to perform a wide range of operations and checks. The
first approach to the development of the checkers was to use this API directly,
without the need of any additional constructs or classes. This approach is the
most direct and simple, but it was used only for the simplest pattern, without a
very complex logic behind it.

\subsection{Pattern 1: explicit align/dealign}

One of the first pattern detected was the explicit usage of the
\lstinline{align} and \lstinline{dealign} functions of the Collektive DSL. These
functions are not supposed to be used directly by the developer because they are
already managed by the backend plugin, and their direct usage can lead to
inconsistent behavior and unexpected results. For how the Collektive DSL is
structured, it was not possible to prevent the usage of these functions directly
through the API, so the frontend plugin was put in charge of this task.

\subsubsection{Formal description}

Given two functions \lstinline{align} and \lstinline{dealign} available in the
\lstinline{Aggregate} interface, the pattern is satisfied when any of these
functions are used explicitly in the code. \Cref{lst:p1-example} shows an
example of this pattern.

\lstinputlisting[
  float, 
  language=Kotlin, 
  label={lst:p1-example},
  caption={Example of Pattern 1 detection in code}
]{listings/P1Example.kt}

\subsubsection{Design and Implementation}

One of the type of checkers available among \ac{FIR} checkers is the
\lstinline{FirFunctionCallChecker}, which is just a type alias for a 
\lstinline{FirExpressionChecker} typed with a \lstinline{FirFunctionCall}. 
This type of checker can inspect function calls usages, calling the 
\lstinline{check} method of the checker with a \lstinline{FirFunctionCall}
as parameter. 
%
To performs checks on the function call, we can inspect the properties of this
parameter. In this case we simply need to \textbf{compare the fully qualified
name} of the function with the one of the interested functions.

Getting the fully qualified name of the function is not directly supported by
the \ac{FIR} API, but it is possible to build a simple utility function that
retrieves it. The final implementation of the checker is shown in
\cref{lst:p1-checker}: some portions of the code are omitted for brevity.

\lstinputlisting[
  float, 
  language=Kotlin, 
  label={lst:p1-checker},
  caption={Implementation of the Pattern 1 checker: \lstinline{ExplicitAlignDealign}}
]{listings/P1Checker.kt}

The \lstinline{context} object is an instance of \lstinline{CheckerContext} and is 
used to provide contextual information to the checker.

The \lstinline{reportOn} method of the \lstinline{reporter} object is used to
report a diagnostic when the pattern gets detected. The parameter passed to this
method determine where the diagnostic is gonna be reported (i.e., the position
in the code) and the message shown to the user (in these checkers, specified
inside an object called \lstinline{FirCollektiveErrors}).

\section{2nd approach: declarative and modular API}

Even though the API available in the checker is flexible and already powerful, 
it can be quite verbose even for simple operations like obtaining the fully
qualified name of a function. To simplify the development of the checkers and
make them more modular, a small API was developed to make declarative checks 
on the \ac{FIR} tree. This API is composed of a set of functions with receiver
that can be used to perform common operations on the \ac{FIR} elements, for 
example to check if a Checker context is \textbf{inside an Aggregate function 
or block}. Some of these utilities are shown in \cref{lst:simple-api}.

\lstinputlisting[
  float, 
  language=Kotlin, 
  label={lst:simple-api},
  caption={Some utility functions of the simple API developed}
]{listings/SimpleApi.kt}


\subsection{Pattern 2: simple aggregate operations in loops}

The second pattern that was inspected is the usage of \emph{aggregate operations
inside loops}. This pattern is more complex because it reflects cases that are
not always inappropriate usages, but if not properly handled can lead to
problems during the alignment phase. 
%
When an aggregate construct (i.e., a function call that needs alignment between
devices) is called inside a loop (e.g. a \lstinline{for} loop), the alignment of
the computation fails because multiple devices are not able to align to the same
``instance'' of the call between the iterations. 
%
However, the alignment can succeed if the loop contains a \emph{custom alignment
operation}, that can be done using the \lstinline{alignedOn} method, which is
available inside the \lstinline{Aggregate} interface. This method accepts an
anonymous function that will be aligned. 
%
Again, this behavior cannot be captured through the internal DSL alone, so the
frontend plugin is needed to handle this case. 

It could seem that this pattern is not overly complex to detect compared to the
previous one, but in reality just adding one more simple constraint makes the
range of possible cases that can be captured following this pattern much broader
and more varied. Consider the cases presented in \cref{lst:p2-corner-case}. 
\lstinputlisting[
  float, 
  language=Kotlin, 
  label={lst:p2-corner-case},
  caption={Corner case related to Pattern 2, where the construct is used inside 
  a nested function}
]{listings/P2CornerCase.kt}
These corner cases are exceptions to the general rule that we previously stated,
in fact:
\begin{itemize}
  \item In the first case, although, technically, the construct is ``present''
  inside the loop, it is not directly called by the loop itself, but by a nested
  function. Note that even though this can be seen as a corner case, this is
  valid Kotlin code and might appear, maybe as slightly modified versions of
  this one, in real use cases.
  \item In the second case, we have the required \lstinline{alignedOn}
  operation, but it is placed outside the loop, making the alignment operation
  useless.
  \item In the third case, the loop is done without alignment, but this is in
  fact correct because the construct is not iterated by the loop since a new 
  aggregate instance is created at each iteration.
\end{itemize}

Many more cases regarding this pattern can be found, but some of these will be
presented as another, separated pattern in \cref{sec:p6}. For now, we will deal
with the simpler cases, described in the following, more formal description.

\subsubsection{Formal description}

Given a construct that loops through various iterations (e.g., a \lstinline{for}
statement, an anonymous function called when cycling collection elements like
\lstinline{map} or \lstinline{forEach} etc.) \emph{inside} an ``aggregate'' block,
the pattern is satisfied when an aggregate construct is called inside this loop
without a wrapping \lstinline{alignedOn} operation \emph{inside the loop} or as
part of a static declaration (e.g., a nested function) whose use is not iterated
by the loop.

\subsubsection{Design and Implementation}

The description might still seem incomplete or vague to some extent, and the
reason is that some potential cases are still not captured. For now, though, the
developed checker will be able to capture the most common cases, and the rest,
as we will see, will be easily integrated when covering the Pattern 6. 

One possible way to approach this pattern is reasoning in terms of
\textbf{containing blocks}: starting from the aggregate function called, we
could be able to determine if the pattern is detected or not just by looking at
the \textbf{sorted list of containing blocks of the function call}. Starting
from there, we first inspect if a loop is, in fact, containing the function
call, and if so, we check for other elements like the presence of the
\lstinline{alignedOn} in the correct order. This procedure is summarized in
\cref{fig:p2-flowchart}.

\begin{figure}
  \centering
  \includegraphics[width=.6\linewidth]{figures/p2-flowchart.pdf}
  \caption{Flowchart representing the procedure to detect Pattern 2}
  \label{fig:p2-flowchart}
\end{figure}

As we can see from the flowchart, the procedure ends immediately if the function
name is one of \lstinline{alignedOn}, \lstinline{align} or \lstinline{dealign},
since these are functions that do not require alignment. Thanks to Kotlin 
nullability system, we can perform these subsequent checks in a declarative way,
adding utilities to the simple API that we previously introduced and making it 
more extensible for more checks like this.
%
We first add these simple functions shown in \cref{lst:p2-api} to the API, and
then we can finally implement the checker, as shown in \cref{lst:p2-checker}. 

\lstinputlisting[
  float, 
  language=Kotlin, 
  label={lst:p2-api},
  caption={Utility functions added to help in the detection of Pattern 2}
]{listings/P2SimpleApi.kt}

\lstinputlisting[
  float, 
  language=Kotlin, 
  label={lst:p2-checker},
  caption={Implementation of the Pattern 2 checker: \lstinline{NoAlignInsideLoop}}
]{listings/P2Checker.kt}

As we can already see, implementing a checker with this approach has several
advantages: the code is readable and modular, and the logic is
declarative and easier to understand. This approach is also extensible and
can be easily adapted to other patterns that require similar checks.
%
This is, however, a very particular case that can be caught simply by looking at
containing elements. Once the pattern becomes more complex, adding concepts like
\emph{symbol's usage references} or \emph{structured operations with particular
requirements}, like nested anonymous calls or simply subsequent statements that
appear in the code that need to be checked together, this approach quickly
becomes less effective and more verbose, as it requires a very effective and
carefully designed API that cover as many cases as possible. 
%
It becomes quite clear that covering corner cases and patterns that reflect many
possible \ac{FIR} trees needs an approach that is more suitable to this kind of
data structures.

\section{3rd approach: visitor pattern}

\subsection{Pattern 3: unnecessary Yielding usage}

\subsection{Pattern 4: unnecessary construct usage}

\section{4th approach: mixed approach}

\subsection{Pattern 5: improper Evolve construct usage}

\subsection{Pattern 6: complex aggregate operations in loops} \label{sec:p6}

%----------------------------------------------------------------------------------------
\chapter{Evaluation and Testing}
\label{chap:evaluation}
%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
\chapter{Conclusion and future works}
\label{chap:conclusion}
%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
% BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\backmatter

\nocite{*} % Remove this as soon as you have the first citation

\bibliographystyle{alpha}
\bibliography{bibliography}

\begin{acknowledgements} % this is optional
Optional. Max 1 page.
\end{acknowledgements}

\end{document}
